% compile with xelatex
\documentclass{article} 
\usepackage{polyglossia} 
\usepackage{amsmath}
\usepackage{fontspec} 
\usepackage{lipsum} 
\usepackage[margin=1in]{geometry}
\usepackage{graphicx} 
\usepackage{caption} 
\usepackage{subcaption}
\usepackage{hyperref} 
\usepackage{booktabs}
%\usepackage{listing}


%%% Metadata and link colors %%% 
\hypersetup{% 
    colorlinks=true, linkcolor=magenta, filecolor=yellow,      
    urlcolor=blue, 
    pdfinfo = {%
        Title = Εργασία Βαθιάς Μάθησης
        Author = {Χρήστος Μάριος Περδίκης},
        Producer = XeLaTeX
    } 
}

%%% Title, language %%% 
\title{Ανάλυση Ιατρικών Εικόνων MedMNIST με CNN, Transfer Learning \& Vision Transformers (PyTorch)}
\date{Εργασία Βαθιάς Μάθησης Χειμερινό Εξάμηνο 2025-2026}
\author{Χρήστος Μάριος Περδίκης 10075 cperdikis@ece.auth.gr}

%%% font stuff, languages, document appearance %%% 
\setmainlanguage{greek}     % this is super useful, automatically changes captions "Listing 1" and "Table 1" to "Σχήμα 1" and "Πίνακας 1". Super useful stuff.
\setotherlanguage{english}

% % % However the following are still necessary for hyperref, because it doesn't officially support the greek language yet, as far as I know anyway % % %
\renewcommand{\equationautorefname}{Εξίσωση}
\renewcommand{\footnoteautorefname}{υποσημείωση}%
\renewcommand{\itemautorefname}{στοιχείο}%
\renewcommand{\figureautorefname}{Σχήμα}%
\renewcommand{\tableautorefname}{Πίνακα}%
\renewcommand{\partautorefname}{Μέρος}%
\renewcommand{\appendixautorefname}{Παράρτημα}%
\renewcommand{\chapterautorefname}{κεφάλαιο}%
\renewcommand{\sectionautorefname}{ενότητα}%
\renewcommand{\subsectionautorefname}{υποενότητα}%
\renewcommand{\subsubsectionautorefname}{υπο-υποενότητα}%
\renewcommand{\paragraphautorefname}{παράγραφος}%
\renewcommand{\subparagraphautorefname}{υποπαράγραφος}%
\renewcommand{\FancyVerbLineautorefname}{γραμμή}%
\renewcommand{\theoremautorefname}{Θεώρημα}%
\renewcommand{\pageautorefname}{σελίδα}%
% % % However the above are still necessary for hyperref, because it doesn't officially support the greek language yet, as far as I know anyway % % %

\setmainfont{CMU Serif}
%\setmainfont{FreeSans}
%\newfontfamily\greekfonttt[Script=Greek]{FreeMono Bold}[SizeFeatures={Size=9}] 

\setlength{\parindent}{0pt}
\setlength{\parskip}{1em}

%%% custom commands and variable %%% 
\NewDocumentCommand{\datasetPicSize}{}{28}
\NewDocumentCommand{\datasetPlithosEikonwn}{}{112,120}
\NewDocumentCommand{\datasetTrainingPlithos}{}{78,468}
\NewDocumentCommand{\datasetValidationPlithos}{}{11,219}
\NewDocumentCommand{\datasetTestPlithos}{}{22,433}
\NewDocumentCommand{\trainingOverTotalPerc}{}{69.98}
\NewDocumentCommand{\validationOverTotalPerc}{}{10}
\NewDocumentCommand{\testOverTotalPerc}{}{20.01}

\begin{document} 
\maketitle 

\section{Περιγραφή dataset}
Το dataset που χρησιμοποιήθηκε είναι το ChestMNIST της MedMNIST. Το ChestMNIST είναι βασισμένο στο NIH-ChestXray14 dataset. 
Αποτελείται από συνολικά από \datasetPlithosEikonwn{} ασπρόμαυρες εικόνες. Το μέγεθος των εικόνων είναι 
$\datasetPicSize{}\times{}\datasetPicSize{}$ ενώ οι αρχικές εικόνες του NIH-ChestXray14 dataset είχαν μέγεθος 
$1024\times{}1024$. Το dataset χωρίζεται σε training,
validation και test splits. Στο training split ανήκουν \datasetTrainingPlithos{} εικόνες, δηλαδή το
$\frac{\datasetTrainingPlithos{}}{\datasetPlithosEikonwn{}} = \trainingOverTotalPerc{}\%$ των συνολικών δεδομένων. 
Στο validation split ανήκουν \datasetValidationPlithos{} εικόνες δηλαδή το 
$\frac{\datasetValidationPlithos{}}{\datasetPlithosEikonwn{}} = 
\validationOverTotalPerc{}\%$ των συνολικών δεδομένων. Στο test split ανήκουν \datasetTestPlithos{} εικόνες δηλαδή το 
$\frac{\datasetTestPlithos{}}{\datasetPlithosEikonwn{}} = \testOverTotalPerc{}\%$ των συνολικών δεδομένων. 

Οι κλάσεις στις οποίες είναι χωρισμένο το dataset περιγράφουν κοινές παθήσεις στην περιοχή του θώρακα, είτε στους πνεύμονες
ή στην καρδιά.  Ακολουθεί λίστα με τις κλάσεις και σύντομη περιγραφή της κάθε πάθησης, καθώς και πώς αυτή εμφανίζεται σε μια
ακτινογραφία θώρακος:
\begin{itemize}
    \item '0': 'atelectasis'. Η Ατελεκτασία	είναι μερική ή πλήρης σύμπτωση ενός πνεύμονα ή ενός λοβού του πνεύμονα, 
        συχνά λόγω απόφραξης (όπως βλέννα) ή πίεσης έξω από τον πνεύμονα Σε ακτινογραφία εμφανίζεται ως	αυξημένη πυκνότητα 
        (λευκότητα) στην επηρεαζόμενη περιοχή.
    \item '1': 'cardiomegaly'.  Η Καρδιομεγαλία χαρακτιρίζεται από τη διεύρυνση της καρδιάς, μπορεί να είναι σημάδι καρδιακής 
        ανεπάρκειας, προβλημάτων βαλβίδων ή υψηλής αρτηριακής πίεσης. Σε μια ακτινογραφία φαίνεται η σιλουέτα της καρδιάς να είναι 
        διευρυμένη.
    \item '2': 'effusion'. Η Πλευριτική Συλλογή αναφέρεται σε μια μη φυσιολογική συσσώρευση υγρού στον πλευ\-ριτικό χώρο 
        (ο χώρος μεταξύ του πνεύμονα και του θωρακικού τοιχώματος). Εμφανίζεται ως αμβλύτητα στις οξείες γωνίες μεταξύ των 
        πλευρών και του διαφράγματος, ένα σημείο μηνίσκου (καμπύλο άνω όριο του υγρού) και αυξημένη λευκότητα.
    \item '3': 'infiltration'. Η Διήθηση είναι ένας μη ειδικός όρος που χρησιμοποιείται συχνά για να περιγράψει οποιαδήποτε ουσία 
        που είναι πυκνότερη από τον αέρα και γεμίζει ένα τμήμα του πνεύμονα. Εμφανίζεται ως μια ασαφής ή κηλιδωτή περιοχή αυξημένης 
        λευκότητας.
    \item '4': 'mass'. Αναφέρεται σε μια μεγάλη, εστιακή πνευμονική βλάβη ή μη φυσιολογική ανάπτυξη. Τα όριά της (ομαλά ή λοβωτά) 
        και η θέση της (κεντρική ή περιφερειακή) είναι βασικά χαρακτηριστικά.
    \item '5': 'nodule'. Το Οζίδιο είναι μια μικρή, εστιακή πνευμονική βλάβη ή μη φυσιολογική ανάπτυξη, που ορίζεται τυπικά ως $<3 cm$ 
        σε διάμετρο. Εμφανίζεται ως μια μικρή, στρογγυλή ή οβάλ ακτινοσκιάνωση (λευκότητα). Χαρακτηριστικά όπως η ασβεστοποίηση ή 
        ο ρυθμός ανάπτυξης βοηθούν στη διάκριση καλοήθων από κακοήθη ευρήματα.
    \item '6': 'pneumonia'. Η Πνευμονία είναι φλεγμονή του πνεύμονα που προκαλείται κυρίως από λοίμωξη, όπου οι κυψελίδες 
        (αεροθάλαμοι) γεμίζουν με πύον και υγρό. Μπορεί να εμφανιστεί ως συμπαγοποίηση (στερε\-οποίηση του πνευμονικού ιστού) σε λοβώδη 
        κατανομή, ή ως πιο κηλιδωτές διηθήσεις.
    \item '7': 'pneumothorax'. Ο Πνευμοθώρακας χαρακτηρίζεται από Αέρα στον πλευριτικό χώρο, προκαλώντας μερική ή πλήρη 
        κατάρρευση του πνεύμονα. Εμφανίζεται ως μια ορατή, λεπτή λευκή γραμμή, με απουσία πνευμονικών αγγειακών σκιάσεων 
        (υπερδιαφάνεια/μαύρο).
    \item '8': 'consolidation'. Η Συμπαγοποίηση είναι μια διαδικασία όπου ο κυψελιδικός αέρας αντικαθίσταται από υγρό, φλεγμονώδες 
        εξίδρωμα ή άλλα προϊόντα (π.χ. σε πνευμονία). Εμφανίζεται ως μια περιοχή ομοιογενούς λευκότητας που δεν προκαλεί 
        απώλεια όγκου. Ο βρογχικός αέρας είναι ορατός εντός της αδιαφανούς περιοχής, κάτι που ονομάζεται αεροβρογχόγραμμα.
    \item '9': 'edema'. Το Πνευμονικό Οίδημα αναφέρεται στην υπερβολική συσσώρευση υγρού στους αεροθάλαμους  και το διάμεσο του πνεύμονα, 
        συχνά λόγω καρδιακής ανεπάρκειας. Εμφανίζεται ως λευκότητες που περιγράφονται ως μοτίβο φτερών νυχτερίδας ή πεταλούδας.
    \item '10': 'emphysema'. Το Εμφύσημα είναι ένας τύπος ΧΑΠ (Χρόνιας Αποφρακτικής Πνευμονοπάθειας) που χαρακτηρίζεται από την 
        καταστροφή των αεροθάλαμων, οδηγώντας σε διευρυμένους αεροχώρους και μειωμένη ανταλλαγή αερίων. 
        Εμφανίζεται ως μειωμένες αγγειακές σκιάσεις, και μερικές φορές παρουσία φυσαλίδων.
    \item '11': 'fibrosis'. Η Ίνωση περιγράφει την ουλοποίηση του πνευμονικού ιστού, συχνά ως αποτέλεσμα χρόνιας φλεγμονής, 
        καθιστώντας τον πνεύμονα δύσκαμπτο και λιγότερο ικανό να διασταλεί. Εμφανίζεται ως δικτυωτές ή γραμμικές λευκότητες, 
        μερικές φορές μαζί με μικρούς κυστικούς αερόχωρους.
    \item '12': 'pleural'. Η Πλευριτική Πάχυνση  χρησιμοποιείται συχνά ως γενικός όρος για μη ειδική πλευριτική νόσο, που είναι 
        ουλές της επένδυσης του πνεύμονα/θωρακικού τοιχώματος. Μπορεί να εμφανιστεί ως γραμμικές ακτινοσκιάνσεις κατά μήκος του 
        εσωτερικού θωρακικού τοιχώματος.
    \item '13': 'hernia'. Η Διαφραγματοκήλη συμβαίνει όταν ένα κοιλιακό όργανο (όπως το στομάχι ή τα έντερα) προβάλλει μέσω ενός 
        ανοίγματος στο διάφραγμα μέσα στη θωρακική κοιλότητα. Η εμφάνιση στην ακτινο\-γραφία εξαρτάται από το περιεχόμενο, αλλά 
        συχνά παρουσιάζεται ως μια μάζα ή αεροφόρος δομή πάνω από το διάφραγμα.
\end{itemize}
Στo~\autoref{pic_montage} υπάρχουν μερικά παραδείγματα εικόνων. 

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{pic_montage.png}
    \caption{Παραδείγματα εικόνων \datasetPicSize{}x\datasetPicSize{} ακτινογραφίας θώρακα από το dataset ChestMNIST}\label{pic_montage}
\end{figure}

Στον~\autoref{class_counts} υπάρχει ο αριθμός των εικόνων
που ανήκει σε κάθε κλάση. Φαίνεται ότι στην κλάση `hernia' ανήκουν πολύ λιγότερα δείγματα από τις υπόλοιπες (μόνο 227), ενώ 
στις κλάσεις `infiltration', `effusion' και `atelectasis' ανήκουν πολύ περισσότερα δείγματα ($11535-19870$). Βλέπουμε ότι
υπάρχει μεγάλη ανισορροπία στον αριθμό των δειγμάτων ανά κλάση και άρα αναμένουμε το δίκτυό μας να δυσκολευτεί με την σωστή πρόβλεψη
της μειοψηφικής κλάσης. Αξίζει να σημειωθεί ότι το ChestMNIST είναι multi-label dataset, δηλαδή μια εικόνα μπορεί να ανήκει
σε παραπάνω από μια κλάσεις ταυτόχρονα και μερικές εικόνες μπορεί να μην ανήκουν και σε καμία κλάση. Στον~\autoref{auc_acc_various_models} 
φαίνονται οι επιδόσεις διαφόρων δημοφιλών μεθόδων στο dataset ChestMNIST. Περισσότερες
πληροφορίες για το evaluation των MedMNIST datasets υπάρχουν στο αντίστοιχο \href{https://www.nature.com/articles/s41597-022-01721-8}{paper}.
Βλέπουμε ότι όλα επιτυγχάνουν αρκετά υψηλό ACC (accuracy) και χαμηλότερο, αλλά ακόμα αποδεκτό AUC (Area Under the Curve). Το γεγονός
ότι το AUC είναι χαμηλότερο από το ACC, ενώ το ACC είναι πολύ υψηλό οφείλεται στην ύπαρξη μειοψηφικών και πλειοψηφικών κλάσεων 
με μεγάλες διαφορές μεταξύ τους. Σε μια τέτοια περίπτωση η μετρική AUC είναι πιο αξιόπιστη για την εκτίμηση της πραγματικής 
επίδοσης της κάθε μεθόδου.

\begin{table}
    \centering
    \begin{tabular}{lc}
        \toprule
        Class Name & Number of occurrences \\
        \midrule
        0 \hspace{1pt} `atelectasis'   & 11535 \\
        1 \hspace{1pt} `cardiomegaly'  & 2772  \\
        2 \hspace{1pt} `effusion'      & 13307 \\
        3 \hspace{1pt} `infiltration'  & 19870 \\
        4 \hspace{1pt} `mass'          & 5746  \\
        5 \hspace{1pt} `nodule'        & 6323  \\
        6 \hspace{1pt} `pneumonia'     & 1353  \\
        7 \hspace{1pt} `pneumothorax'  & 5298  \\
        8 \hspace{1pt} `consolidation' & 4667  \\
        9 \hspace{1pt} `edema'         & 2303  \\
        10 `emphysema'                 & 2516  \\
        11 `fibrosis'                  & 1686  \\
        12 `pleural'                   & 3385  \\
        13 `hernia'                    & 227   \\
        \bottomrule
    \end{tabular}
    \caption{Αριθμός εμφανίσεων της κάθε κλάσης σε όλο το dataset (training και test)}\label{class_counts}
\end{table}

\begin{table}
    \centering
    \begin{tabular}{lcc}
        \toprule
        Method & AUC & ACC \\
        \midrule
        ResNet-18 (28)       & 0.768 & 0.947  \\
        ResNet-18 (224)      & 0.773 & 0.947  \\
        ResNet-50 (28)       & 0.769 & 0.947  \\
        ResNet-50 (224)      & 0.773 & 0.948  \\
        auto-sklearn11       & 0.649 & 0.779  \\
        AutoKeras12          & 0.742 & 0.937  \\
        Google AutoML Vision & 0.778 & 0.948  \\
        \bottomrule
    \end{tabular}
    \caption{Επιδόσεις διαφόρων δημοφιλών μεθόδων στο dataset ChestMNIST, στις μεθόδους ResNet χρησιμοποιήθηκε και μέγεθος φωτογραφίας $228\times{}228$, scaled από εικόνες $28\times{}28$}\label{auc_acc_various_models}
\end{table}

Ακολουθεί η ανάλυση του κώδικα.

\section{CNN From Scratch}
Ο κώδικας της εργασίας υλοποιήθηκε στο περιβάλλον Google Colab και ειναι χωρισμένος σε τρία διαφορετικά
μέρη. Στον κώδικα του πρώτου μέρους καλούμαστε να υλοποιήσουμε ένα CNN δικής μας αρχιτεκτονικής και
να το εκπαιδεύσουμε εμείς. Σε αυτή την εργασία γίνεται πρώτα μια προετοιμασία του dataset, μετά ορίζεται
η αρχιτεκτονική του CNN, γίνεται το training και το validation, το test του καλύτερου μοντέλου και τέλος η παράθεση των αποτελεσμάτων. 

\subsection{Προετοιμασία του Dataset}
Ο κώδικας για την προετοιμασία του dataset είναι παρόμοιος και στα τρία μέρη.
Για να υπάρχει αναπαραγωγιμότη\-τα
θέτουμε manual seeds. Αρχικοποιούμε τα τρία splits του dataset
ChestMNIST (train, validation, test) και τα αντίστοιχα dataloaders με batch size 64. Στο πρώτο μέρος
δεν κάνουμε
κανονικοποί\-ηση ακόμα γιατί όλα τα hyperparameter combinations θα δοκιμαστούν αργότερα με και χωρίς
κανονικοποίηση της εισόδου. Ελέγχουμε πόσες κλάσεις υπάρχουν και ποιά είναι τα ονόματά τους.  
Έπειτα γίνεται μέτρηση του πλήθους των εικόνων που ανήκουν σε κάθε κλάση. Ο υπολογισμός έγινε και 
για ολόκληρο το dataset (και τα τρία split συγχωνευμένα) αλλά και μόνο για το training set. Για
κάθε split του dataset σχηματίζουμε ένα array όπου για γραμμές έχει το label κάθε εικόνας και 
έπειτα προσθέτουμε κατακόρυφα. Εφόσον τα labels υποδεικνύονται με one-hot encoding, οι τελικοί 
αριθμοί είναι το πλήθος των εικόνων που ανήκουν σε κάθε κλάση. Αυτή η μέθοδος μας διευκολύνει
γιατί όπως προαναφέρθηκε έχουμε multi-label classification problem, δηλαδή μια εικόνα μπορεί να
ανήκει σε παραπάνω από μια κλάσεις ή και σε καμία. Έπειτα επιβεβαιώνουμε ότι οι εικόνες 
μας είναι ασπρόμαυρες. Παίρνουμε 
το πρώτο στοιχείο του validation split (το οποίο δεν γίνεται shuffled, οπότε θα πρέπει να βλέπουμε
την ίδια εικόνα κάθε φορά), διαχωρίζουμε τα δεδομένα της εικόνας από τα δεδομένα των labels και
τότε μπορούμε εύκολα να επιβεβαιώσουμε ότι η εικόνα έχει μόνο ένα κανάλι φωτεινότητας για κάθε 
pixel και άρα είναι ασπρόμαυρη. 

Για την κανονικοποίηση των δεδομένων εισόδου, υπολογίζουμε τη μέση τιμή και την τυπική απόκλιση 
των εικόνων του training dataset. Για λόγους εξοινκονόμησης μνήμης χρησιμοποιούμε ένα προσωρινό dataloader 
το οποίο χωρίζει το training dataset σε batches των 256 στοιχείων. Έπειτα προσθέτουμε τις τυπικές
αποκλίσεις και τις μέσες τιμές όλων των batches και διαιρούμε με τον συνολικό αριθμό των samples.


\subsection{Αρχιτεκτονική του CNN}
Στο πρώτο μέρος της εργασίας ορίζουμε ένα Συνελικτικό Νευρωνικό Δίκτυο (CNN) από την αρχή. Η 
αρχιτεκτο\-νική του έχει ως εξής. Αποτελείται από 3 συνελικτικά blocks, ένα dropout layer και
ένα τελευταίο fully connected layer. Κάθε συνελικτικό block αποτελείται από μια συνέλιξη,
ένα layer κανονικοποίησης με ReLU 
συνάρτηση ενεργοποίησης και ένα max pooling layer. Το πρώτο convolutional layer έχει 
32 εξόδους (άρα 32 βάρη, άρα 32 φίλτρα), το δεύτερο 64 εξόδους και το τρίτο 128. Όλα τα
layers έχουν μέγεθος kernel 3x3, $stride=1$ και $padding=1$. Η επιλογή του $padding=1$ βασίστηκε 
στο ότι, αν έιχαμε $padding=0$, σύμφωνα με τον τύπο.\label{architecture_CNN_from_scratch}

\begin{equation}
    out\_dim = \frac{(in\_dim - krnl\_size + 2*padding)}{stride} + 1
\end{equation}
Για $padding=0$ στο πρώτο block ($in\_dim=28$):
\begin{align*}
    out\_dim &= \frac{(28 - 3 + 2*0)}{1} + 1 \\
    out\_dim &= 26
\end{align*}
Βλέπουμε ότι έξοδος είναι μικρότερη από την είσοδο. Αυτό έχει ως συνέπεια το feature map
να χάνει πληροφορία σε κάθε convolutional layer. Αυτό, εφόσον έχουμε ήδη 
ελεγχόμενη απώλεια πληροφορίας με τα max pooling layers, δεν είναι 
επιθυμητό. Ενώ για $padding=1$:
\begin{align*}
    out\_dim &= \frac{(28 - 3 + 2*1)}{1} + 1 \\
    out\_dim &= 28
\end{align*}
Η έξοδος του convolutional layer έχει τις ίδιες διαστάσεις με την είσοδο.
Το dropout layer, όπως και τα layers κανονικοποίησης των συνελικτικών 
blocks, ενεργοποιούνται μόνο όταν είναι ορισμένη η αντίστοιχη υπερπαράμετρος.
Τέλος, πριν τα τελικά dropout και fully-connected layers, το output του τρίτου 
συνελικτικού block γίνεται flatten.

Για την αυτοματοποίηση της διαδικασίας ορίζεται μία λίστα συνδυασμών υπερπαραμέτρων. 
Οι υπερπαράμετροι που ορίζονται και οι πιθανές τους τιμές βρίσκονται στον~\autoref{hyperparameter_combinations}.
Ένας συνδυασμός αυτών των παραμέτρων είναι ένα στοιχείο της λίστας.

\begin{table}
    \centering
    \begin{tabular}{cc}
        Όνομα Υπερπαραμέτρου & Αποδεκτές Τιμές \\
        \toprule
        normalization & `batchnorm' ή `layernorm' ή None \\
        \midrule
        dropout & Δεκαδικός Αριθμός ή None \\   
        \midrule
        weight decay & Δεκαδικός Αριθμός ή None \\
        \midrule
        learning rate & Δεκαδικός Αριθμός ή None \\
        \midrule
        input normalization & True ή False \\
        \bottomrule
    \end{tabular}
    \caption{Υπερπαράμετροι που ορίζονται στη λίστα συνδυασμών υπερπαραμέτρων και οι πιθανές τιμές τους}\label{hyperparameter_combinations}
\end{table}
Η τιμή `normalization' ορίζει αν εφαρμοστεί layer ή batch normalization στα convolutional blocks του CNN. Το dropout ορίζει το
dropout rate του dropout layer. Μετά από δοκιμές παρατηρήθηκε ότι οι τιμές $0.5$ και $0.7$ έχουν καλύτερες επιδώσεις από 
dropout rate $0.2$, για αυτό και η τελευταία δεν χρησιμοποιήθηκε στην τελική λίστα συνδυασμών υπερπαραμέτρων. Η υπερπαράμετρος
`weight decay' ορίζει το ρυθμό weight decay του Adam Optimizer. Παρατηρήθηκε ότι οι τιμές $10^3$ και $10^4$ είχαν καλύτερη επίδοση
από την τιμή $10^2$ και έτσι μόνοι οι $10^3$ και $10^4$ χρησιμοποιήθηκαν στην τελική λίστα συνδυασμών υπερπαραμέτρων. 
Το learning rate επηρεάζει το learning rate του Adam Optimizer και το input normalization καθορίζει αν θα γίνει
κανονικοποίηση εισόδου ή όχι. Η τελική λίστα συνδυασμών υπερπαραμέτρων κατέληξε να έχει 30 συνδυασμούς.


\subsection{Training, Validation, Testing και Αποτελέσματα}
Με τρόπο παρόμοιο που μας υποδείχθηκε στο εργαστήριο διαμορφώνεται ο κώδικας για το training, validation και
testing. Οι συναρτήσεις train\_one\_epoch, evaluate\_model και test\_model όλες εκπαιδεύουν, επαληθεύουν και 
τεστάρουν το μοντέλο αντίστοιχα για ένα μόνο epoch. 

Όταν καλείται η συνάρτηση train\_one\_epoch, αρχικοποιούνται οι μεταβλητές που θα χρησιμοποιηθούν στον υπολογισμό
του accuracy και του loss και ενεργοποιείται το train mode του μοντέλου. Τα δεδομένα εισόδου μετακινούνται στην 
κατάλληλη συσκευή (CPU ή GPU), γίνεται το forward pass, υπολογίζεται το loss με το criterion BCEWithLogitsLoss (ορίζεται
παρακάτω), υπολογίζονται τα gradients με το backward pass και ενημερώνονται τα βάρη του μοντέλου με τη χρήση Adam
Optimizer (επίσης ορίζεται παρακάτω). Εφόσον έχουμε multi-label classification problem, ο υπολογισμός του accuracy
είναι λίγο πιο περίπλοκος. Τις εξόδους του μοντέλου τις περνάμε από μια σιγμοειδή συνάρτηση, έτσι ώστε να μετατραπούν
από logits σε πιθανότητες. Έτσι η έξοδος για μια εικόνα έχει $14$ πιθανότητες που υποδεικνύουν πόσο πιθανό
είναι αυτή η εικόνα να ανήκει σε κάθε κλάση. θεωρούμε αυθαίρετα ότι αν η πιθανότητα μιας κλάσης είναι πάνω 
από $0.5$, τότε η εικόνα ανήκει και σε αυτήν την κλάση. Μετατρέπουμε την πιθανότητα που ανήκει στο δίαστημα $\left[0,1\right]$
σε $1.0$ (True), αν ανήκει στο $\left[0.5,1\right]$ και σε $0.0$ (False) αν ανήκει στο $\left[0, 0.5\right]$. Έτσι μπορούμε
από εδώ και πέρα να υπολογίσουμε την ακρίβεια όπως θα την υπολογίζαμε σε ένα binary classification problem, διαιρόντας
τον αριθμό σωστών προβλέψεων προς τον αριθμό συνολικών προβλέψεων. Για να υπολογίσουμε τη συνολική απώλεια, προσθέτουμε την απώλεια κάθε 
batch στο running\_loss και αφού τελειώσει το training διαιρούμε το running\_loss με τον αριθμό των δειγμάτων στο 
training dataset. Η συνάρτηση train\_one\_epoch επιστρέφει τη συνολική απώλεια και την ακρίβεια.

Η συνάρτηση evaluate\_model είναι πολύ παρόμοια με την train\_one\_epoch. Οι μόνες διαφορές τις είναι ότι το μοντέλο
μπαίνει σε λειτουργία evaluation και δεν γίνεται backprop του σφάλματος στο δίκτυο, ούτε καλείται ο Adam Optimizer για
ανανέωση των βάρων του δικτύου. Ο υπολογισμός της απώλειας και της ακρίβειας γίνεται με τον ίδιο τρόπο.

Η συνάρτηση test\_model με τη σειρά της μοιάζει με την evaluate\_model, αλλά διαφέρει στον υπολογισμό της ακρίβειας
και στο ότι υπολογίζονται τα confusion matrices. Επειδή χρειάζεται να υπολογίσουμε τα confusion matrices πρέπει να
κρατήσουμε τις πληροφορίες όλων των labels της εισόδου και όλων των labels που προέβλεψε το μοντέλο. Εφόσον έχουμε 
όλα τα labels που προέβλεψε το μοντέλο μας έτοιμα, αρκεί να συγκρίνουμε τα all\_predictions και all\_labels arrays element-wise
και να πάρουμε την μέση τιμή όλων των συγκρίσεων για να υπολογίσουμε το accuracy. Το array all\_predictions έχει ήδη μετατραπεί σε
δυαδική πιθανότητα True ή False, όπως περιγράψαμε για την συνάρτηση train\_one\_epoch, πριν τη σύγκριση. Στον~\autoref{confusion_matrix}
φαίνεται η δομή ενός confusion matrix. True Negatives αναφέρεται στις φορές που η πρόβλεψη είναι σωστή στο ότι μία εικόνα δεν ανήκει
σε αυτήν την κλάση. Αντίστοιχα η τιμή του False Negatives είναι οι φορές που η πρόβλεψη είναι λάθος στο ότι δεν ανήκει η εικόνα
στη συγκεκριμένη κλάση. Για τον υπολογισμό των confusion matrices συγκρίνουμε τις προβλέψεις του μοντέλου με τα πραγματικά labels για κάθε κλάση
και προσθέτουμε $1$ στην αντίστοιχη κατηγορία σε κάθε σύγκριση. Η έξοδος της test\_model είναι το συνολικό accuracy και μια λίστα με 
όλα τα confusion matrices.

\begin{table}
    \centering
    \begin{tabular}{|cc|}
            {True Negatives} & {False Positives} \\
            {False Negatives} & {True Positives} \\
    \end{tabular}
    \caption{Confusion Matrix}\label{confusion_matrix}
\end{table}

Οι παραπάνω συναρτήσεις καλούνται για 30 φορές (30 epochs) για κάθε hyperparameter combination. Αν η αντίστοιχη υπερπαράμετρος είναι 
ορισμένη κανονικοποιούνται τα δεδομένα της εισόδου με τα transformations που ορίστηκαν στην~\autoref{architecture_CNN_from_scratch}.
Εδώ ορίζονται και ο optimizer να είναι Adam Optimizer και το loss function να είναι το BCELogitsWithLoss. Η επιλογή του loss function
έγινε διότι η Binary Cross Entropy έχει την καλύτερη επίδοση σε multi-label classification problems. Για κάθε hyperparameter
combination, κάθε φορά που το accuracy που υπολογίζεται από την evaluate model είναι μεγαλύτερο από το προηγούμενο μέγιστο, αποθηκεύεται
η κατάσταση του μοντέλου. Εφόσον το μέγιστο accuracy μπορεί να επιτευχθεί πριν το τελευταίο epoch, αυτή είναι μια μορφή early stopping.
Έπειτα αν αυτό το accuracy είναι μεγαλύτερο από τα accuracy των υπόλοιπων hyperparameter combinations, ο τωρινός συνδυασμός υπερπαραμέτρων
αποθηκεύεται ως ο καλύτερος και ανακτάται η κατάσταση του μοντέλου όταν επιτεύχθηκε αυτό το accuracy. Για να είναι δυνατή η προβολή 
των training και validation plots με accuracy και loss για τα 30 epochs, τα απαραίτητα δεδομένα αποθηκεύονται σε λίστες. 

Αφότου δοκιμαστούν όλα τα hyperparameter combinations, φορτώνεται η κατάσταση του μοντέλου που πέτυχε το καλύτερο accuracy και
ο αντίστοιχος συνδυασμός υπερπαραμέτρων. Αυτό το μοντέλο τρέχει μία φορά με είσοδο το test\_dataset, το οποίο ανάλογα με την 
αντίστοιχη υπερπαράμετρο είτε θα κανονικοποιηθεί πρώτα είτε όχι. Τέλος, σχηματίζονται γραφικές παραστάσεις με τα training και validation 
accuracy και loss για τα τριάντα epochs για κάθε hyperparameter που δοκιμάστηκε. 

\section{Συμπεράσματα}
Ο συνδυασμός με την καλύτερη επίδοση ήταν αυτός με batch normalization, καθόλου dropout και weight decay $10^3$, learning rate $10^4$ 
και με input normalizationα. Αυτός ο συνδυασμός πέτυχε 0.9496 validation accuracy, 0.1602 validation loss στο 25ο epoch και 0.9477 accuracy στο test
dataset, αριθμοί ικανοποιητικά υψηλοί εκ πρώτης όψεως. Μπορούμε να πούμε με αυτό το υψηλό test accuracy ότι με αυτές τις υπερπαραμέτρους
το μοντέλο μας γενίκευσε την πληροφορία που έμαθε γενικά καλά. Στην~\autoref{best_combination_plot} 
φαίνεται το γράφημα με την ακρίβεια και την απώλεια αυτού του συνδυασμού. Στον~\autoref{confusion_matrices} υπάρχουν τα confusion matrices 
από το test dataset. Όταν λάβουμε υπόψιν μας τα confusion matrices όμως η αρχική θετική 
εικόνα που έχουμε για την επίδοση του μοντέλου μας με βάση την ακρίβεια δεν είναι απόλυτα ακριβής. 
Παρατηρούμε στα confusion matrices ότι η μεγάλη ακρίβεια του μοντέλου οφείλεται στον μεγάλο αριθμό
True Negatives σε όλες τις κλάσεις. Αντιθέτως, στις περισσότερες κλάσεις το μοντέλο δεν έχει ούτε ένα True Positive, 
δηλαδή δεν κατάφερε να προβλέψει σωστά ούτε ούτε μια φορά τις συγκεκριμένες παθήσεις. Ακόμα και στις κλάσεις
που τα True Positives δεν είναι μηδέν, ο αριθμός των False Negatives είναι πολύ μεγαλύτερος, όπως για παράδειγμα στην 
κλάση `inflitration', όπου έχουμε 22 True Positives και 3916 False Negatives. Αυτό θα αποτελούσε μεγάλο πρόβλημα αν θέλαμε να 
εφαρμόσουμε το μοντέλο μας σε περιπτώσεις πραγματικής ζωής, καθώς δεν προβλέπει παθήσεις που θα έπρεπε να ανιχνεύει. Τέλος,
βλέπουμε ότι σε όλες τις κλάσεις έχουμε πολύ χαμηλό αριθμό False Positives. Αυτό, σε συνδυασμό με τα υπόλοιπα δεδομένα από τα
confusion matrices, μας δείχνει ότι η ανισορροπία των κλάσεων επηρεάζει σε πολύ μεγάλο βαθμό την συμπεριφορά του μοντέλου μας. 
Το μοντέλο έμαθε να δίνει προτεραιότητα στο να προβλέπει την απώλεια πάθησης, η οποία φαίνεται να είναι η πλειοψηφική κλάση, 
με σκοπό να βελτιστοποιεί το accuracy. Αυτό έρχεται σε κόστος των υπόλοιπων, μειοψηφικών κλάσεων, καθώς το μοντέλο δεν έγινε αρκετά
ευαίσθητο για να ανιχνεύσει την παρουσία τους. Με βάση τα παραπάνω, συμπεραίνουμε ότι σε multi-label classification προβλήματα
με υψηλή ανισορροπία κλάσεων οι συγκεντρωτικές μετρικές όπως το συνολικό accuracy και loss μπορεί να είναι παραπλανητικές για την εκτίμηση της πρακτικής επίδοσης του μοντέλου μας. 
Σε τέτοιες περιπτώσεις τα confusion matrices είναι απαραίτητοι για την κατανόηση της πρακτικής χρησιμότητας και των ειδικών τρόπων αποτυχίας 
ενός μοντέλου.

\begin{figure}
    \includegraphics[width=\linewidth]{images/input_norm_yes_plot_best.png}
    \caption{Γραφική παράσταση για τον καλύτερο συνδυασμό υπερπαραμέτρων. Αριστερά είναι η ακρίβεια για training και validation και δεξιά η απώλεια.}
    \label{best_combination_plot}
\end{figure}


\begin{table}
    \centering
    \caption{Confusion Matrices για όλες τις κλάσεις.}\label{confusion_matrices}
    % --- Row 1 ---
    \begin{minipage}{0.3\textwidth}
        \centering
        \caption*{Κλάση 'atelectasis' (0)}
        \begin{tabular}{| c | c |}
            \hline
            {{19962}} & {{51}} \\ \hline
            {{2385}} & {{35}} \\ \hline
        \end{tabular}
    \end{minipage}
    \hfill
    \begin{minipage}{0.3\textwidth}
        \centering
        \caption*{Κλάση 'cardiomegaly' (1)}
        \begin{tabular}{| c | c |}
            \hline
            {{21831}} & {{20}} \\ \hline
            {{553}} & {{29}} \\ \hline
        \end{tabular}
    \end{minipage}
    \hfill
    \begin{minipage}{0.3\textwidth}
        \centering
        \caption*{Κλάση 'effusion' (2)}
        \begin{tabular}{| c | c |}
            \hline
            {{19555}} & {{124}} \\ \hline
            {{2527}} & {{227}} \\ \hline
        \end{tabular}
        \caption*{}
    \end{minipage}


    % --- Row 2 ---
    \vspace{1em}
    \begin{minipage}{0.3\textwidth}
        \centering
        \caption*{Κλάση 'infiltration' (3)}
        \begin{tabular}{| c | c |}
            \hline
            {{18471}} & {{24}} \\ \hline
            {{3916}} & {{22}} \\ \hline
        \end{tabular}
    \end{minipage}
    \hfill
    \begin{minipage}{0.3\textwidth}
        \centering
        \caption*{Κλάση 'mass' (4)}
        \begin{tabular}{| c | c |}
            \hline
            {{21299}} & {{1}} \\ \hline
            {{1133}} & {{0}} \\ \hline
        \end{tabular}
    \end{minipage}
    \hfill
    \begin{minipage}{0.3\textwidth}
        \centering
        \caption*{Κλάση 'nodule' (5)}
        \begin{tabular}{| c | c |}
            \hline
            {{21098}} & {{0}} \\ \hline
            {{1335}} & {{0}} \\ \hline
        \end{tabular}
    \end{minipage}

    \vspace{1em}

    % --- Row 3 ---
    \begin{minipage}{0.3\textwidth}
        \centering
        \caption*{Κλάση 'pneumonia' (6)}
        \begin{tabular}{| c | c |}
            \hline
            {{22191}} & {{0}} \\ \hline
            {{242}} & {{0}} \\ \hline
        \end{tabular}
    \end{minipage}
    \hfill
    \begin{minipage}{0.3\textwidth}
        \centering
        \caption*{Κλάση 'pneumothorax' (7)}
        \begin{tabular}{| c | c |}
            \hline
            {{21344}} & {{0}} \\ \hline
            {{1089}} & {{0}} \\ \hline
        \end{tabular}
    \end{minipage}
    \hfill
    \begin{minipage}{0.3\textwidth}
        \centering
        \caption*{Κλάση 'consolidation' (8)}
        \begin{tabular}{| c | c |}
            \hline
            {{21476}} & {{0}} \\ \hline
            {{957}} & {{0}} \\ \hline
        \end{tabular}
    \end{minipage}

    % --- Row 4 ---
    \vspace{1em}
    \begin{minipage}{0.3\textwidth}
        \centering
        \caption*{Κλάση 'edema' (9)}
        \begin{tabular}{| c | c |}
            \hline
            {{22020}} & {{0}} \\ \hline
            {{413}} & {{0}} \\ \hline
        \end{tabular}
    \end{minipage}
    \hfill
    \begin{minipage}{0.3\textwidth}
        \centering
        \caption*{Κλάση 'emphysema' (10)}
        \begin{tabular}{| c | c |}
            \hline
            {{21924}} & {{0}} \\ \hline
            {{509}} & {{0}} \\ \hline
        \end{tabular}
    \end{minipage}
    \hfill
    \begin{minipage}{0.3\textwidth}
        \centering
        \caption*{Κλάση 'fibrosis' (11)}
        \begin{tabular}{| c | c |}
            \hline
            {{22071}} & {{0}} \\ \hline
            {{362}} & {{0}} \\ \hline
        \end{tabular}
    \end{minipage}

    \vspace{1em}

    % --- Row 5 ---
    \begin{minipage}{0.45\textwidth}
        \centering
        \caption*{Κλάση 'pleural' (12)}
        \begin{tabular}{| c | c |}
            \hline
            {{21699}} & {{0}} \\ \hline
            {{734}} & {{0}} \\ \hline
        \end{tabular}
    \end{minipage}
    \hfill
    \begin{minipage}{0.45\textwidth}
        \centering
        \caption*{Κλάση 'hernia' (13)}
        \begin{tabular}{| c | c |}
            \hline
            {{22391}} & {{0}} \\ \hline
            {{42}} & {{0}} \\ \hline
        \end{tabular}
    \end{minipage}

    \vspace{1em}

    % --- Row 6 ---
    \begin{minipage}{0.3\textwidth}
        \centering
        \caption*{Δείγμα Confusion Matrix για ευκολία}
        \begin{tabular}{| c | c |}
            \hline
            \text{{True Negative}} & \text{{False Positive}} \\ \hline
            \text{{False Negative}} & \text{{True Positive}} \\ \hline
        \end{tabular}
    \end{minipage}
\end{table}


Με βάση τα γενικότερα αποτελέσματα καταλήγουμε σε διάφορα συμπεράσματα. Η εκπαίδευση του μοντέλου μας δεν είναι δεδομένο ότι πάντα 
βελτίωνε την επίδοσή του. Ανάλογα με την επιλογή των υπερπαραμέτρων μπορεί η επίδοσή του να γίνεται χειρότερη όσο συνεχίζει η εκπαίδευση.
Ένα παράδειγμα για αυτό το φαινόμενο φαίνεται στην~\autoref{bad_training}, όπου πολύ γρήγορα το accuracy του validation πέφτει, το loss 
του validation ανεβαίνει, ενώ η επίδοση του μοντέλου στο training dataset βελτιώνεται. Είναι πολύ πιθανό το μοντέλο μας να `μάθαινε υπερβολικά καλά', 
να έκανε overfitting στα δεδομένα του training dataset με τον συγκεκριμένο συνδυασμό υπερπαραμέτρων και να μην μπορούσε να γενικεύσει σε άλλη είσοδο.
Από αυτό συμπεραίνουμε ότι η επιλογή των σωστών υπερπαραμέτρων είναι πολύ σημαντική και επηρεάζει την επίδοση του μοντέλου μας. Η εφαρμογή input normalization 
δεν επηρέασε κατά πολύ την επίδοση του μοντέλου και χωρίς input normalization η επίδοση του μοντέλου μας ήταν λίγο καλύτερη.

\begin{figure}
    \includegraphics[width=\linewidth]{images/input_norm_yes_layer_norm_bad.png}
    \caption{Παράδειγμα κακής γενίκευσης μοντέλου και overfitting στο training dataset}
    \label{bad_training}
\end{figure}

\textbf{TODO}:
\begin{itemize}
    \item σύγκριση input normalization και όχι input normalization. Νέος καλύτερος συνδυασμός υπερπαραμέτρων χωρίς input normalization.
\end{itemize}

\section{Transfer Learning}
Στο δεύτερο μέρος της εργασίας καλούμαστε να χρησιμοποιήσουμε ένα pre-trained model και να εξετάσουμε πότε αξίζει να κάνουμε feature extraction και πότε 
fine-tuning. Για αυτό τον σκοπό επιλέχθηκε το μοντέλο MobileNetV2 λόγω της μικρών υπολογιστικών απαιτήσεών του.

\subsection{Προετοιμασία του Dataset}
Ο κώδικας για την προετοιμασία του dataset, ορισμού seeds για αναπαραγωγιμότητα και των συναρτήσεων train\_one\_epoch, evaluate\_model και test\_model παραμένει επί το πλείστον ο ίδιος με τον κώδικα του πρώτου μέρους.
Η μόνη διαφορά είναι στο ότι οι εικόνες της εισόδου μετατρέπονται από ασπρόμαυρες σε `έγχρωμες', δηλαδή αντί για ένα κανάλι φωτεινότητας κάθε εικόνα έχει τρία κανάλια. Αυτό 
είναι απαίτηση του μοντέλου που επιλέξαμε καθώς περιμένει έγχρωμες εικόνες στην είσοδό του. Το batch size εξακολουθεί να είναι 64.

\subsection{Περιγραφή Pre-Trained Μοντέλου}
\subsection{Training, Validation, Testing και Αποτελέσματα}

\section{Small Vision Transformer}
\subsection{Προετοιμασία του Dataset}
\subsection{DeiT-tiny Transformer}
\subsection{Training, Validation, Testing και Αποτελέσματα}
\section{Συμπεράσματα}
\subsection{Προτάσεις Βελτίωσης}
Part1:
\begin{itemize}
    \item Περισσότερα hyperparameter combinations.
    \item Αρχικοποίηση βαρών.
    \item Επιλογή βέλτιστου συνδυασμού υπερπαραμέτρων με βάση μετρικές όπως AUC και F1 score, όχι του accuracy, λόγω ανισορροπίας κλάσεων.
    \item Data Augmentation στις κλάσεις με πολύ λιγότερα δείγματα με σκοπό την καταπολέμησης της ανισορροπίας.
\end{itemize}

Part2:
\begin{itemize}
    \item Χρήση pre-trained μοντέλο με καλύτερη επίδοση σε πιο περίπλοκα προβλήματα (VGG16, ResNet18), εφόσον υπάρχουν οι πόροι.
\end{itemize}


%%% ΚΕΦΑΛΑΙΟ 2 - CNN FROM SCRATCH %%%
%• Σχεδιάστε ένα CNN με τουλάχιστον 3 συνελικτικά blocks, κάθε block να περιλαμβάνει
%Conv2d → ReLU → MaxPooling2d. Προτείνεται να ξεκινήσετε με 32, 64 και 128 φίλτρα
%αντίστοιχα, kernel size 3×3. 
%• Προσθέστε ένα fully connected layer στο τέλος με αριθμό νευρώνων ίσο με τον αριθμό
%των κλάσεων.
%• Πειραματιστείτε με Batch Normalization μετά από κάθε Conv2d και/ή Layer
%Normalization στο τέλος κάθε block.
%• Δοκιμάστε Dropout στο fully connected layer με ποσοστά, 0.2, 0.5 και 0.7.
%• Χρησιμοποιήστε Weight Decay (L2 regularization) με τιμές 1e-4, 1e-3 και 1e-2.
%• Χρησιμοποιήστε Adam optimizer, με learning rate ενδεικτικά 1e-3, αλλά μπορείτε να
%πειραματιστείτε με τιμές μεταξύ 1e-4 και 5e-3.
%    o Χωρίς normalization ή dropout: 1e-3
%    o Με BatchNorm ή LayerNorm: 5e-4
%    o Με Dropout ή Weight Decay: 1e-3 ή 1e-4 ανάλογα με το συνδυασμό τεχνικών
%• Batch size: 64, Epochs: 30–50.
%• Παρακολουθήστε training και validation curves (loss & accuracy) για κάθε συνδυασμό
%τεχνικών.
%• Υπολογίστε test accuracy και δημιουργήστε confusion matrix.
%• Συγκρίνετε τα αποτελέσματα και σχολιάστε πώς οι διαφορετικές τεχνικές επηρεάζουν
%την απόδοση και τη γενίκευση.

%If your model performs worse with normalized data compared to non-normalized data (as you observed in some input_normalization: False combinations leading to better validation accuracy), it could mean a few things:

%   The Model Doesn't Benefit: For your specific SimpleCNN architecture and the ChestMNIST dataset, perhaps the range of pixel values (0-1
%   after ToTensor) is already sufficient, or the model is robust enough that further scaling (normalization with mean/std) doesn't provide a
%   significant advantage, or even slightly hinders its learning by shifting the input distribution in a way that the network's initial weights
%   aren't well-suited for. Convolutional layers can sometimes be relatively robust to the scale of input features within a reasonable range.  

%   Dataset Characteristics: The distribution of pixel values in ChestMNIST might be such that ToTensor (which scales pixels to [0, 1]) is already
%   the most beneficial preprocessing step, and further Normalize (scaling to mean 0, std 1) doesn't add value or might even create a less optimal
%   input distribution for the specific model you've designed.   

%   Other Hyperparameters: It's also possible that the other hyperparameters
%   (learning rate, weight decay, normalization layers, dropout) are not optimally tuned to leverage the benefits of input normalization. A model
%   that benefits from normalization might require different learning rates or other architectural choices.



%%% ΚΕΦΑΛΑΙΟ 1 - ΠΕΡΙΓΡΑΦΗ ΤΟΥ DATASET %%%
%• Περιγράψτε τον τύπο των εικόνων (π.χ. grayscale ή RGB) και την ανάλυση (π.χ. 28×28 ή 32×32). - done
%• Αναφέρετε τον αριθμό των κλάσεων και τη σημασία τους (π.χ. τύποι όγκων, κατηγορίες ιστών, κλπ.). - done
%• Καταγράψτε το μέγεθος του dataset (train/test/validation splits) και αν υπάρχει ανισορροπία στις κλάσεις. - done, να περιγράψω τον τρόπο με τον οποίο βρήκα των αριθμό των εικόνων σε κάθε κλάση;
%• Συμπεριλάβετε μερικά παραδείγματα εικόνων. - done, μήπως να βάλω παράδεγμα μια εικόνα που ανήκει σε κάθε κλάση;

\vspace{3em}
\centering
\emph{*** ΤΕΛΟΣ ΑΝΑΦΟΡΑΣ ***}
\end{document}
