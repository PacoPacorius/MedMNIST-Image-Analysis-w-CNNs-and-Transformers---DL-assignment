% compile with xelatex
\documentclass{article} 
\usepackage{polyglossia} 
\usepackage{amsmath}
\usepackage{fontspec} 
\usepackage{lipsum} 
\usepackage[margin=1in]{geometry}
\usepackage{graphicx} 
\usepackage{caption} 
\usepackage{subcaption}
\usepackage{hyperref} 
\usepackage{booktabs}
%\usepackage{listing}


%%% Metadata and link colors %%% 
\hypersetup{% 
    colorlinks=true, linkcolor=magenta, filecolor=yellow,      
    urlcolor=blue, 
    pdfinfo = {%
        Title = Εργασία Βαθιάς Μάθησης
        Author = {Χρήστος Μάριος Περδίκης},
        Producer = XeLaTeX
    } 
}

%%% Title, language %%% 
\title{Ανάλυση Ιατρικών Εικόνων MedMNIST με CNN, Transfer Learning \& Vision Transformers (PyTorch)}
\date{Εργασία Βαθιάς Μάθησης Χειμερινό Εξάμηνο 2025-2026}
\author{Χρήστος Μάριος Περδίκης 10075 cperdikis@ece.auth.gr}

%%% font stuff, languages, document appearance %%% 
\setmainlanguage{greek}     % this is super useful, automatically changes captions "Listing 1" and "Table 1" to "Σχήμα 1" and "Πίνακας 1". Super useful stuff.
\setotherlanguage{english}

% % % However the following are still necessary for hyperref, because it doesn't officially support the greek language yet, as far as I know anyway % % %
\renewcommand{\equationautorefname}{Εξίσωση}
\renewcommand{\footnoteautorefname}{υποσημείωση}%
\renewcommand{\itemautorefname}{στοιχείο}%
\renewcommand{\figureautorefname}{Σχήμα}%
\renewcommand{\tableautorefname}{Πίνακα}%
\renewcommand{\partautorefname}{Μέρος}%
\renewcommand{\appendixautorefname}{Παράρτημα}%
\renewcommand{\chapterautorefname}{κεφάλαιο}%
\renewcommand{\sectionautorefname}{ενότητα}%
\renewcommand{\subsectionautorefname}{υποενότητα}%
\renewcommand{\subsubsectionautorefname}{υπο-υποενότητα}%
\renewcommand{\paragraphautorefname}{παράγραφος}%
\renewcommand{\subparagraphautorefname}{υποπαράγραφος}%
\renewcommand{\FancyVerbLineautorefname}{γραμμή}%
\renewcommand{\theoremautorefname}{Θεώρημα}%
\renewcommand{\pageautorefname}{σελίδα}%
% % % However the above are still necessary for hyperref, because it doesn't officially support the greek language yet, as far as I know anyway % % %

\setmainfont{CMU Serif}
%\setmainfont{FreeSans}
\newfontfamily\greekfonttt[Script=Greek]{FreeMono Bold}[SizeFeatures={Size=9}] 

\setlength{\parindent}{0pt}
\setlength{\parskip}{1em}

%%% custom commands and variable %%% 
\NewDocumentCommand{\datasetPicSize}{}{28}
\NewDocumentCommand{\datasetPlithosEikonwn}{}{112,120}
\NewDocumentCommand{\datasetTrainingPlithos}{}{78,468}
\NewDocumentCommand{\datasetValidationPlithos}{}{11,219}
\NewDocumentCommand{\datasetTestPlithos}{}{22,433}
\NewDocumentCommand{\trainingOverTotalPerc}{}{69.98}
\NewDocumentCommand{\validationOverTotalPerc}{}{10}
\NewDocumentCommand{\testOverTotalPerc}{}{20.01}

\begin{document} 
\maketitle 

\section{Περιγραφή dataset}
Το dataset που χρησιμοποιήθηκε είναι το ChestMNIST της MedMNIST. Το ChestMNIST είναι βασισμένο στο NIH-ChestXray14 dataset. 
Αποτελείται από συνολικά από \datasetPlithosEikonwn{} ασπρόμαυρες εικόνες. Το μέγεθος των εικόνων είναι 
$\datasetPicSize{}\times{}\datasetPicSize{}$ ενώ οι αρχικές εικόνες του NIH-ChestXray14 dataset είχαν μέγεθος 
$1024\times{}1024$. Το dataset χωρίζεται σε training,
validation και test splits. Στο training split ανήκουν \datasetTrainingPlithos{} εικόνες, δηλαδή το
$\frac{\datasetTrainingPlithos{}}{\datasetPlithosEikonwn{}} = \trainingOverTotalPerc{}\%$ των συνολικών δεδομένων. 
Στο validation split ανήκουν \datasetValidationPlithos{} εικόνες δηλαδή το 
$\frac{\datasetValidationPlithos{}}{\datasetPlithosEikonwn{}} = 
\validationOverTotalPerc{}\%$ των συνολικών δεδομένων. Στο test split ανήκουν \datasetTestPlithos{} εικόνες δηλαδή το 
$\frac{\datasetTestPlithos{}}{\datasetPlithosEikonwn{}} = \testOverTotalPerc{}\%$ των συνολικών δεδομένων. 

Οι κλάσεις στις οποίες είναι χωρισμένο το dataset περιγράφουν κοινές παθήσεις στην περιοχή του θώρακα, είτε στους πνεύμονες
ή στην καρδιά.  Ακολουθεί λίστα με τις κλάσεις και σύντομη περιγραφή της κάθε πάθησης, καθώς και πώς αυτή εμφανίζεται σε μια
ακτινογραφία θώρακος:
\begin{itemize}
    \item '0': 'atelectasis'. Η Ατελεκτασία	είναι μερική ή πλήρης σύμπτωση ενός πνεύμονα ή ενός λοβού του πνεύμονα, 
        συχνά λόγω απόφραξης (όπως βλέννα) ή πίεσης έξω από τον πνεύμονα Σε ακτινογραφία εμφανίζεται ως	αυξημένη πυκνότητα 
        (λευκότητα) στην επηρεαζόμενη περιοχή.
    \item '1': 'cardiomegaly'.  Η Καρδιομεγαλία χαρακτιρίζεται από τη διεύρυνση της καρδιάς, μπορεί να είναι σημάδι καρδιακής 
        ανεπάρκειας, προβλημάτων βαλβίδων ή υψηλής αρτηριακής πίεσης. Σε μια ακτινογραφία φαίνεται η σιλουέτα της καρδιάς να είναι 
        διευρυμένη.
    \item '2': 'effusion'. Η Πλευριτική Συλλογή αναφέρεται σε μια μη φυσιολογική συσσώρευση υγρού στον πλευ\-ριτικό χώρο 
        (ο χώρος μεταξύ του πνεύμονα και του θωρακικού τοιχώματος). Εμφανίζεται ως αμβλύτητα στις οξείες γωνίες μεταξύ των 
        πλευρών και του διαφράγματος, ένα σημείο μηνίσκου (καμπύλο άνω όριο του υγρού) και αυξημένη λευκότητα.
    \item '3': 'infiltration'. Η Διήθηση είναι ένας μη ειδικός όρος που χρησιμοποιείται συχνά για να περιγράψει οποιαδήποτε ουσία 
        που είναι πυκνότερη από τον αέρα και γεμίζει ένα τμήμα του πνεύμονα. Εμφανίζεται ως μια ασαφής ή κηλιδωτή περιοχή αυξημένης 
        λευκότητας.
    \item '4': 'mass'. Αναφέρεται σε μια μεγάλη, εστιακή πνευμονική βλάβη ή μη φυσιολογική ανάπτυξη. Τα όριά της (ομαλά ή λοβωτά) 
        και η θέση της (κεντρική ή περιφερειακή) είναι βασικά χαρακτηριστικά.
    \item '5': 'nodule'. Το Οζίδιο είναι μια μικρή, εστιακή πνευμονική βλάβη ή μη φυσιολογική ανάπτυξη, που ορίζεται τυπικά ως $<3 cm$ 
        σε διάμετρο. Εμφανίζεται ως μια μικρή, στρογγυλή ή οβάλ ακτινοσκιάνωση (λευκότητα). Χαρακτηριστικά όπως η ασβεστοποίηση ή 
        ο ρυθμός ανάπτυξης βοηθούν στη διάκριση καλοήθων από κακοήθη ευρήματα.
    \item '6': 'pneumonia'. Η Πνευμονία είναι φλεγμονή του πνεύμονα που προκαλείται κυρίως από λοίμωξη, όπου οι κυψελίδες 
        (αεροθάλαμοι) γεμίζουν με πύον και υγρό. Μπορεί να εμφανιστεί ως συμπαγοποίηση (στερε\-οποίηση του πνευμονικού ιστού) σε λοβώδη 
        κατανομή, ή ως πιο κηλιδωτές διηθήσεις.
    \item '7': 'pneumothorax'. Ο Πνευμοθώρακας χαρακτηρίζεται από Αέρα στον πλευριτικό χώρο, προκαλώντας μερική ή πλήρη 
        κατάρρευση του πνεύμονα. Εμφανίζεται ως μια ορατή, λεπτή λευκή γραμμή, με απουσία πνευμονικών αγγειακών σκιάσεων 
        (υπερδιαφάνεια/μαύρο).
    \item '8': 'consolidation'. Η Συμπαγοποίηση είναι μια διαδικασία όπου ο κυψελιδικός αέρας αντικαθίσταται από υγρό, φλεγμονώδες 
        εξίδρωμα ή άλλα προϊόντα (π.χ. σε πνευμονία). Εμφανίζεται ως μια περιοχή ομοιογενούς λευκότητας που δεν προκαλεί 
        απώλεια όγκου. Ο βρογχικός αέρας είναι ορατός εντός της αδιαφανούς περιοχής, κάτι που ονομάζεται αεροβρογχόγραμμα.
    \item '9': 'edema'. Το Πνευμονικό Οίδημα αναφέρεται στην υπερβολική συσσώρευση υγρού στους αεροθάλαμους  και το διάμεσο του πνεύμονα, 
        συχνά λόγω καρδιακής ανεπάρκειας. Εμφανίζεται ως λευκότητες που περιγράφονται ως μοτίβο φτερών νυχτερίδας ή πεταλούδας.
    \item '10': 'emphysema'. Το Εμφύσημα είναι ένας τύπος ΧΑΠ (Χρόνιας Αποφρακτικής Πνευμονοπάθειας) που χαρακτηρίζεται από την 
        καταστροφή των αεροθάλαμων, οδηγώντας σε διευρυμένους αεροχώρους και μειωμένη ανταλλαγή αερίων. 
        Εμφανίζεται ως μειωμένες αγγειακές σκιάσεις, και μερικές φορές παρουσία φυσαλίδων.
    \item '11': 'fibrosis'. Η Ίνωση περιγράφει την ουλοποίηση του πνευμονικού ιστού, συχνά ως αποτέλεσμα χρόνιας φλεγμονής, 
        καθιστώντας τον πνεύμονα δύσκαμπτο και λιγότερο ικανό να διασταλεί. Εμφανίζεται ως δικτυωτές ή γραμμικές λευκότητες, 
        μερικές φορές μαζί με μικρούς κυστικούς αερόχωρους.
    \item '12': 'pleural'. Η Πλευριτική Πάχυνση  χρησιμοποιείται συχνά ως γενικός όρος για μη ειδική πλευριτική νόσο, που είναι 
        ουλές της επένδυσης του πνεύμονα/θωρακικού τοιχώματος. Μπορεί να εμφανιστεί ως γραμμικές ακτινοσκιάνσεις κατά μήκος του 
        εσωτερικού θωρακικού τοιχώματος.
    \item '13': 'hernia'. Η Διαφραγματοκήλη συμβαίνει όταν ένα κοιλιακό όργανο (όπως το στομάχι ή τα έντερα) προβάλλει μέσω ενός 
        ανοίγματος στο διάφραγμα μέσα στη θωρακική κοιλότητα. Η εμφάνιση στην ακτινο\-γραφία εξαρτάται από το περιεχόμενο, αλλά 
        συχνά παρουσιάζεται ως μια μάζα ή αεροφόρος δομή πάνω από το διάφραγμα.
\end{itemize}
Στo~\autoref{pic_montage} υπάρχουν μερικά παραδείγματα εικόνων. 

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{pic_montage.png}
    \caption{Παραδείγματα εικόνων \datasetPicSize{}x\datasetPicSize{} ακτινογραφίας θώρακα από το dataset ChestMNIST}\label{pic_montage}
\end{figure}

Στον~\autoref{class_counts} υπάρχει ο αριθμός των εικόνων
που ανήκει σε κάθε κλάση. Φαίνεται ότι στην κλάση `hernia' ανήκουν πολύ λιγότερα δείγματα από τις υπόλοιπες (μόνο 227), ενώ 
στις κλάσεις `infiltration', `effusion' και `atelectasis' ανήκουν πολύ περισσότερα δείγματα ($11535-19870$). Βλέπουμε ότι
υπάρχει μεγάλη ανισορροπία στον αριθμό των δειγμάτων ανά κλάση και άρα αναμένουμε το δίκτυό μας να δυσκολευτεί με την σωστή πρόβλεψη
της μειοψηφικής κλάσης. Αξίζει να σημειωθεί ότι το ChestMNIST είναι multi-label dataset, δηλαδή μια εικόνα μπορεί να ανήκει
σε παραπάνω από μια κλάσεις ταυτόχρονα και μερικές εικόνες μπορεί να μην ανήκουν και σε καμία κλάση. Στον~\autoref{auc_acc_various_models} 
φαίνονται οι επιδόσεις διαφόρων δημοφιλών μεθόδων στο dataset ChestMNIST. Περισσότερες
πληροφορίες για το evaluation των MedMNIST datasets υπάρχουν στο αντίστοιχο \href{https://www.nature.com/articles/s41597-022-01721-8}{paper}.
Βλέπουμε ότι όλα επιτυγχάνουν αρκετά υψηλό ACC (accuracy) και χαμηλότερο, αλλά ακόμα αποδεκτό AUC (Area Under the Curve). Το γεγονός
ότι το AUC είναι χαμηλότερο από το ACC, ενώ το ACC είναι πολύ υψηλό οφείλεται στην ύπαρξη μειοψηφικών και πλειοψηφικών κλάσεων 
με μεγάλες διαφορές μεταξύ τους. Σε μια τέτοια περίπτωση η μετρική AUC είναι πιο αξιόπιστη για την εκτίμηση της πραγματικής 
επίδοσης της κάθε μεθόδου.

\begin{table}
    \centering
    \begin{tabular}{lc}
        \toprule
        Class Name & Number of occurrences \\
        \midrule
        0 \hspace{1pt} `atelectasis'   & 11535 \\
        1 \hspace{1pt} `cardiomegaly'  & 2772  \\
        2 \hspace{1pt} `effusion'      & 13307 \\
        3 \hspace{1pt} `infiltration'  & 19870 \\
        4 \hspace{1pt} `mass'          & 5746  \\
        5 \hspace{1pt} `nodule'        & 6323  \\
        6 \hspace{1pt} `pneumonia'     & 1353  \\
        7 \hspace{1pt} `pneumothorax'  & 5298  \\
        8 \hspace{1pt} `consolidation' & 4667  \\
        9 \hspace{1pt} `edema'         & 2303  \\
        10 `emphysema'                 & 2516  \\
        11 `fibrosis'                  & 1686  \\
        12 `pleural'                   & 3385  \\
        13 `hernia'                    & 227   \\
        \bottomrule
    \end{tabular}
    \caption{Αριθμός εμφανίσεων της κάθε κλάσης σε όλο το dataset (training και test)}\label{class_counts}
\end{table}

\begin{table}
    \centering
    \begin{tabular}{lcc}
        \toprule
        Method & AUC & ACC \\
        \midrule
        ResNet-18 (28)       & 0.768 & 0.947  \\
        ResNet-18 (224)      & 0.773 & 0.947  \\
        ResNet-50 (28)       & 0.769 & 0.947  \\
        ResNet-50 (224)      & 0.773 & 0.948  \\
        auto-sklearn11       & 0.649 & 0.779  \\
        AutoKeras12          & 0.742 & 0.937  \\
        Google AutoML Vision & 0.778 & 0.948  \\
        \bottomrule
    \end{tabular}
    \caption{Επιδόσεις διαφόρων δημοφιλών μεθόδων στο dataset ChestMNIST, στις μεθόδους ResNet χρησιμοποιήθηκε και μέγεθος φωτογραφίας $228\times{}228$, scaled από εικόνες $28\times{}28$}\label{auc_acc_various_models}
\end{table}

Ακολουθεί η ανάλυση του κώδικα.

\section{CNN From Scratch}
Ο κώδικας της εργασίας υλοποιήθηκε στο περιβάλλον Google Colab και ειναι χωρισμένος σε τρία διαφορετικά
μέρη. Στον κώδικα του πρώτου μέρους καλούμαστε να υλοποιήσουμε ένα CNN δικής μας αρχιτεκτονικής και
να το εκπαιδεύσουμε εμείς. Σε αυτή την εργασία γίνεται πρώτα μια προετοιμασία του dataset, μετά ορίζεται
η αρχιτεκτονική του CNN, γίνεται το training και το validation, το test του καλύτερου μοντέλου και τέλος η παράθεση των αποτελεσμάτων. 

\subsection{Προετοιμασία του Dataset}
Ο κώδικας για την προετοιμασία του dataset είναι παρόμοιος και στα τρία μέρη.
Για να υπάρχει αναπαραγωγιμότη\-τα
θέτουμε manual seeds. Αρχικοποιούμε τα τρία splits του dataset
ChestMNIST (train, validation, test) και τα αντίστοιχα dataloaders με batch size 64. Στο πρώτο μέρος
δεν κάνουμε
κανονικοποί\-ηση ακόμα γιατί όλα τα hyperparameter combinations θα δοκιμαστούν αργότερα με και χωρίς
κανονικοποίηση της εισόδου. Ελέγχουμε πόσες κλάσεις υπάρχουν και ποιά είναι τα ονόματά τους.  
Έπειτα γίνεται μέτρηση του πλήθους των εικόνων που ανήκουν σε κάθε κλάση. Ο υπολογισμός έγινε και 
για ολόκληρο το dataset (και τα τρία split συγχωνευμένα) αλλά και μόνο για το training set. Για
κάθε split του dataset σχηματίζουμε ένα array όπου για γραμμές έχει το label κάθε εικόνας και 
έπειτα προσθέτουμε κατακόρυφα. Εφόσον τα labels υποδεικνύονται με one-hot encoding, οι τελικοί 
αριθμοί είναι το πλήθος των εικόνων που ανήκουν σε κάθε κλάση. Αυτή η μέθοδος μας διευκολύνει
γιατί όπως προαναφέρθηκε έχουμε multi-label classification problem, δηλαδή μια εικόνα μπορεί να
ανήκει σε παραπάνω από μια κλάσεις ή και σε καμία. Έπειτα επιβεβαιώνουμε ότι οι εικόνες 
μας είναι ασπρόμαυρες. Παίρνουμε 
το πρώτο στοιχείο του validation split (το οποίο δεν γίνεται shuffled, οπότε θα πρέπει να βλέπουμε
την ίδια εικόνα κάθε φορά), διαχωρίζουμε τα δεδομένα της εικόνας από τα δεδομένα των labels και
τότε μπορούμε εύκολα να επιβεβαιώσουμε ότι η εικόνα έχει μόνο ένα κανάλι φωτεινότητας για κάθε 
pixel και άρα είναι ασπρόμαυρη. 

Για την κανονικοποίηση των δεδομένων εισόδου, υπολογίζουμε τη μέση τιμή και την τυπική απόκλιση 
των εικόνων του training dataset. Για λόγους εξοινκονόμησης μνήμης χρησιμοποιούμε ένα προσωρινό dataloader 
το οποίο χωρίζει το training dataset σε batches των 256 στοιχείων. Έπειτα προσθέτουμε τις τυπικές
αποκλίσεις και τις μέσες τιμές όλων των batches και διαιρούμε με τον συνολικό αριθμό των samples.


\subsection{Αρχιτεκτονική του CNN}
Στο πρώτο μέρος της εργασίας ορίζουμε ένα Συνελικτικό Νευρωνικό Δίκτυο (CNN) από την αρχή. Η 
αρχιτεκτο\-νική του έχει ως εξής. Αποτελείται από 3 συνελικτικά blocks, ένα dropout layer και
ένα τελευταίο fully connected layer. Κάθε συνελικτικό block αποτελείται από μια συνέλιξη,
ένα layer κανονικοποίησης με ReLU 
συνάρτηση ενεργοποίησης και ένα max pooling layer. Το πρώτο convolutional layer έχει 
32 εξόδους (άρα 32 βάρη, άρα 32 φίλτρα), το δεύτερο 64 εξόδους και το τρίτο 128. Όλα τα
layers έχουν μέγεθος kernel 3x3, $stride=1$ και $padding=1$. Η επιλογή του $padding=1$ βασίστηκε 
στο ότι, αν έιχαμε $padding=0$, σύμφωνα με τον τύπο.\label{architecture_CNN_from_scratch}

\begin{equation}
    out\_dim = \frac{(in\_dim - krnl\_size + 2*padding)}{stride} + 1
\end{equation}
Για $padding=0$ στο πρώτο block ($in\_dim=28$):
\begin{align*}
    out\_dim &= \frac{(28 - 3 + 2*0)}{1} + 1 \\
    out\_dim &= 26
\end{align*}
Βλέπουμε ότι έξοδος είναι μικρότερη από την είσοδο. Αυτό έχει ως συνέπεια το feature map
να χάνει πληροφορία σε κάθε convolutional layer. Αυτό, εφόσον έχουμε ήδη 
ελεγχόμενη απώλεια πληροφορίας με τα max pooling layers, δεν είναι 
επιθυμητό. Ενώ για $padding=1$:
\begin{align*}
    out\_dim &= \frac{(28 - 3 + 2*1)}{1} + 1 \\
    out\_dim &= 28
\end{align*}
Η έξοδος του convolutional layer έχει τις ίδιες διαστάσεις με την είσοδο.
Το dropout layer, όπως και τα layers κανονικοποίησης των συνελικτικών 
blocks, ενεργοποιούνται μόνο όταν είναι ορισμένη η αντίστοιχη υπερπαράμετρος.
Τέλος, πριν τα τελικά dropout και fully-connected layers, το output του τρίτου 
συνελικτικού block γίνεται flatten.

Για την αυτοματοποίηση της διαδικασίας ορίζεται μία λίστα συνδυασμών υπερπαραμέτρων. 
Οι υπερπαράμετροι που ορίζονται και οι πιθανές τους τιμές βρίσκονται στον~\autoref{hyperparameter_combinations}.
Ένας συνδυασμός αυτών των παραμέτρων είναι ένα στοιχείο της λίστας.

\begin{table}
    \centering
    \begin{tabular}{cc}
        Όνομα Υπερπαραμέτρου & Αποδεκτές Τιμές \\
        \toprule
        normalization & `batchnorm' ή `layernorm' ή None \\
        \midrule
        dropout & Δεκαδικός Αριθμός ή None \\   
        \midrule
        weight decay & Δεκαδικός Αριθμός ή None \\
        \midrule
        learning rate & Δεκαδικός Αριθμός ή None \\
        \midrule
        input normalization & True ή False \\
        \bottomrule
    \end{tabular}
    \caption{Υπερπαράμετροι που ορίζονται στη λίστα συνδυασμών υπερπαραμέτρων και οι πιθανές τιμές τους}\label{hyperparameter_combinations}
\end{table}
Η τιμή `normalization' ορίζει αν εφαρμοστεί layer ή batch normalization στα convolutional blocks του CNN. Το dropout ορίζει το
dropout rate του dropout layer. Μετά από δοκιμές παρατηρήθηκε ότι οι τιμές $0.5$ και $0.7$ έχουν καλύτερες επιδώσεις από 
dropout rate $0.2$, για αυτό και η τελευταία δεν χρησιμοποιήθηκε στην τελική λίστα συνδυασμών υπερπαραμέτρων. Η υπερπαράμετρος
`weight decay' ορίζει το ρυθμό weight decay του Adam Optimizer. Παρατηρήθηκε ότι οι τιμές $10^3$ και $10^4$ είχαν καλύτερη επίδοση
από την τιμή $10^2$ και έτσι μόνοι οι $10^3$ και $10^4$ χρησιμοποιήθηκαν στην τελική λίστα συνδυασμών υπερπαραμέτρων. 
Το learning rate επηρεάζει το learning rate του Adam Optimizer και το input normalization καθορίζει αν θα γίνει
κανονικοποίηση εισόδου ή όχι. Η τελική λίστα συνδυασμών υπερπαραμέτρων κατέληξε να έχει 30 συνδυασμούς.


\subsection{Training, Validation, Testing και Αποτελέσματα}
Με τρόπο παρόμοιο που μας υποδείχθηκε στο εργαστήριο διαμορφώνεται ο κώδικας για το training, validation και
testing. Οι συναρτήσεις train\_one\_epoch, evaluate\_model και test\_model όλες εκπαιδεύουν, επαληθεύουν και 
τεστάρουν το μοντέλο αντίστοιχα για ένα μόνο epoch. 

Όταν καλείται η συνάρτηση train\_one\_epoch, αρχικοποιούνται οι μεταβλητές που θα χρησιμοποιηθούν στον υπολογισμό
του accuracy και του loss και ενεργοποιείται το train mode του μοντέλου. Τα δεδομένα εισόδου μετακινούνται στην 
κατάλληλη συσκευή (CPU ή GPU), γίνεται το forward pass, υπολογίζεται το loss με το criterion BCEWithLogitsLoss (ορίζεται
παρακάτω), υπολογίζονται τα gradients με το backward pass και ενημερώνονται τα βάρη του μοντέλου με τη χρήση Adam
Optimizer (επίσης ορίζεται παρακάτω). Εφόσον έχουμε multi-label classification problem, ο υπολογισμός του accuracy
είναι λίγο πιο περίπλοκος. Τις εξόδους του μοντέλου τις περνάμε από μια σιγμοειδή συνάρτηση, έτσι ώστε να μετατραπούν
από logits σε πιθανότητες. Έτσι η έξοδος για μια εικόνα έχει $14$ πιθανότητες που υποδεικνύουν πόσο πιθανό
είναι αυτή η εικόνα να ανήκει σε κάθε κλάση. θεωρούμε αυθαίρετα ότι αν η πιθανότητα μιας κλάσης είναι πάνω 
από $0.5$, τότε η εικόνα ανήκει και σε αυτήν την κλάση. Μετατρέπουμε την πιθανότητα που ανήκει στο δίαστημα $\left[0,1\right]$
σε $1.0$ (True), αν ανήκει στο $\left[0.5,1\right]$ και σε $0.0$ (False) αν ανήκει στο $\left[0, 0.5\right]$. Έτσι μπορούμε
από εδώ και πέρα να υπολογίσουμε την ακρίβεια όπως θα την υπολογίζαμε σε ένα binary classification problem, διαιρόντας
τον αριθμό σωστών προβλέψεων προς τον αριθμό συνολικών προβλέψεων. Για να υπολογίσουμε τη συνολική απώλεια, προσθέτουμε την απώλεια κάθε 
batch στο running\_loss και αφού τελειώσει το training διαιρούμε το running\_loss με τον αριθμό των δειγμάτων στο 
training dataset. Η συνάρτηση train\_one\_epoch επιστρέφει τη συνολική απώλεια και την ακρίβεια.

Η συνάρτηση evaluate\_model είναι πολύ παρόμοια με την train\_one\_epoch. Οι μόνες διαφορές τις είναι ότι το μοντέλο
μπαίνει σε λειτουργία evaluation και δεν γίνεται backprop του σφάλματος στο δίκτυο, ούτε καλείται ο Adam Optimizer για
ανανέωση των βάρων του δικτύου. Ο υπολογισμός της απώλειας και της ακρίβειας γίνεται με τον ίδιο τρόπο.

Η συνάρτηση test\_model με τη σειρά της μοιάζει με την evaluate\_model, αλλά διαφέρει στον υπολογισμό της ακρίβειας
και στο ότι υπολογίζονται τα confusion matrices. Επειδή χρειάζεται να υπολογίσουμε τα confusion matrices πρέπει να
κρατήσουμε τις πληροφορίες όλων των labels της εισόδου και όλων των labels που προέβλεψε το μοντέλο. Εφόσον έχουμε 
όλα τα labels που προέβλεψε το μοντέλο μας έτοιμα, αρκεί να συγκρίνουμε τα all\_predictions και all\_labels arrays element-wise
και να πάρουμε την μέση τιμή όλων των συγκρίσεων για να υπολογίσουμε το accuracy. Το array all\_predictions έχει ήδη μετατραπεί σε
δυαδική πιθανότητα True ή False, όπως περιγράψαμε για την συνάρτηση train\_one\_epoch, πριν τη σύγκριση. Στον~\autoref{confusion_matrix}
φαίνεται η δομή ενός confusion matrix. True Negatives αναφέρεται στις φορές που η πρόβλεψη είναι σωστή στο ότι μία εικόνα δεν ανήκει
σε αυτήν την κλάση. Αντίστοιχα η τιμή του False Negatives είναι οι φορές που η πρόβλεψη είναι λάθος στο ότι δεν ανήκει η εικόνα
στη συγκεκριμένη κλάση. Για τον υπολογισμό των confusion matrices συγκρίνουμε τις προβλέψεις του μοντέλου με τα πραγματικά labels για κάθε κλάση
και προσθέτουμε $1$ στην αντίστοιχη κατηγορία σε κάθε σύγκριση. Η έξοδος της test\_model είναι το συνολικό accuracy και μια λίστα με 
όλα τα confusion matrices.

\begin{table}
    \centering
    \begin{tabular}{|cc|}
            {True Negatives} & {False Positives} \\
            {False Negatives} & {True Positives} \\
    \end{tabular}
    \caption{Confusion Matrix}\label{confusion_matrix}
\end{table}

Οι παραπάνω συναρτήσεις καλούνται για 30 φορές (30 epochs) για κάθε hyperparameter combination. Αν η αντίστοιχη υπερπαράμετρος είναι 
ορισμένη κανονικοποιούνται τα δεδομένα της εισόδου με τα transformations που ορίστηκαν στην~\autoref{architecture_CNN_from_scratch}.
Εδώ ορίζονται και ο optimizer να είναι Adam Optimizer και το loss function να είναι το BCELogitsWithLoss. Η επιλογή του loss function
έγινε διότι η Binary Cross Entropy έχει την καλύτερη επίδοση σε multi-label classification problems. Για κάθε hyperparameter
combination, κάθε φορά που το accuracy που υπολογίζεται από την evaluate model είναι μεγαλύτερο από το προηγούμενο μέγιστο, αποθηκεύεται
η κατάσταση του μοντέλου. Εφόσον το μέγιστο accuracy μπορεί να επιτευχθεί πριν το τελευταίο epoch, αυτή είναι μια μορφή early stopping.
Έπειτα αν αυτό το accuracy είναι μεγαλύτερο από τα accuracy των υπόλοιπων hyperparameter combinations, ο τωρινός συνδυασμός υπερπαραμέτρων
αποθηκεύεται ως ο καλύτερος και ανακτάται η κατάσταση του μοντέλου όταν επιτεύχθηκε αυτό το accuracy. Για να είναι δυνατή η προβολή 
των training και validation plots με accuracy και loss για τα 30 epochs, τα απαραίτητα δεδομένα αποθηκεύονται σε λίστες. 

Αφότου δοκιμαστούν όλα τα hyperparameter combinations, φορτώνεται η κατάσταση του μοντέλου που πέτυχε το καλύτερο accuracy και
ο αντίστοιχος συνδυασμός υπερπαραμέτρων. Αυτό το μοντέλο τρέχει μία φορά με είσοδο το test\_dataset, το οποίο ανάλογα με την 
αντίστοιχη υπερπαράμετρο είτε θα κανονικοποιηθεί πρώτα είτε όχι. Τέλος, σχηματίζονται γραφικές παραστάσεις με τα training και validation 
accuracy και loss για τα τριάντα epochs για κάθε hyperparameter που δοκιμάστηκε. 

\subsection{Συμπεράσματα}
Ο συνδυασμός με την καλύτερη επίδοση ήταν αυτός με batch normalization, καθόλου dropout και weight decay $10^3$, learning rate $10^4$ 
και με input normalizationα. Αυτός ο συνδυασμός πέτυχε 0.9496 validation accuracy, 0.1602 validation loss στο 25ο epoch και 0.9477 accuracy στο test
dataset, αριθμοί ικανοποιητικά υψηλοί εκ πρώτης όψεως. Μπορούμε να πούμε με αυτό το υψηλό test accuracy ότι με αυτές τις υπερπαραμέτρους
το μοντέλο μας γενίκευσε την πληροφορία που έμαθε γενικά καλά. Στην~\autoref{best_combination_plot} 
φαίνεται το γράφημα με την ακρίβεια και την απώλεια αυτού του συνδυασμού. Στον~\autoref{confusion_matrices} υπάρχουν τα confusion matrices 
από το test dataset. Όταν λάβουμε υπόψιν μας τα confusion matrices όμως η αρχική θετική 
εικόνα που έχουμε για την επίδοση του μοντέλου μας με βάση την ακρίβεια δεν είναι απόλυτα ακριβής. 
Παρατηρούμε στα confusion matrices ότι η μεγάλη ακρίβεια του μοντέλου οφείλεται στον μεγάλο αριθμό
True Negatives σε όλες τις κλάσεις. Αντιθέτως, στις περισσότερες κλάσεις το μοντέλο δεν έχει ούτε ένα True Positive, 
δηλαδή δεν κατάφερε να προβλέψει σωστά ούτε ούτε μια φορά τις συγκεκριμένες παθήσεις. Ακόμα και στις κλάσεις
που τα True Positives δεν είναι μηδέν, ο αριθμός των False Negatives είναι πολύ μεγαλύτερος, όπως για παράδειγμα στην 
κλάση `inflitration', όπου έχουμε 22 True Positives και 3916 False Negatives. Αυτό θα αποτελούσε μεγάλο πρόβλημα αν θέλαμε να 
εφαρμόσουμε το μοντέλο μας σε περιπτώσεις πραγματικής ζωής, καθώς δεν προβλέπει παθήσεις που θα έπρεπε να ανιχνεύει. Τέλος,
βλέπουμε ότι σε όλες τις κλάσεις έχουμε πολύ χαμηλό αριθμό False Positives. Αυτό, σε συνδυασμό με τα υπόλοιπα δεδομένα από τα
confusion matrices, μας δείχνει ότι η ανισορροπία των κλάσεων επηρεάζει σε πολύ μεγάλο βαθμό την συμπεριφορά του μοντέλου μας. 
Το μοντέλο έμαθε να δίνει προτεραιότητα στο να προβλέπει την απώλεια πάθησης, η οποία φαίνεται να είναι η πλειοψηφική κλάση, 
με σκοπό να βελτιστοποιεί το accuracy. Αυτό έρχεται σε κόστος των υπόλοιπων, μειοψηφικών κλάσεων, καθώς το μοντέλο δεν έγινε αρκετά
ευαίσθητο για να ανιχνεύσει την παρουσία τους. Με βάση τα παραπάνω, συμπεραίνουμε ότι σε multi-label classification προβλήματα
με υψηλή ανισορροπία κλάσεων οι συγκεντρωτικές μετρικές όπως το συνολικό accuracy και loss μπορεί να είναι παραπλανητικές για την εκτίμηση της πρακτικής επίδοσης του μοντέλου μας. 
Σε τέτοιες περιπτώσεις τα confusion matrices είναι απαραίτητα για την κατανόηση της πρακτικής χρησιμότητας και των ειδικών τρόπων αποτυχίας 
ενός μοντέλου.

\begin{figure}
    \includegraphics[width=\linewidth]{images/input_norm_yes_plot_best.png}
    \caption{Γραφική παράσταση για τον καλύτερο συνδυασμό υπερπαραμέτρων. Αριστερά είναι η ακρίβεια για training και validation και δεξιά η απώλεια.}
    \label{best_combination_plot}
\end{figure}


\begin{table}
    \centering
    \caption{Confusion Matrices για όλες τις κλάσεις.}\label{confusion_matrices}
    % --- Row 1 ---
    \begin{minipage}{0.3\textwidth}
        \centering
        \caption*{Κλάση 'atelectasis' (0)}
        \begin{tabular}{| c | c |}
            \hline
            {{19962}} & {{51}} \\ \hline
            {{2385}} & {{35}} \\ \hline
        \end{tabular}
    \end{minipage}
    \hfill
    \begin{minipage}{0.3\textwidth}
        \centering
        \caption*{Κλάση 'cardiomegaly' (1)}
        \begin{tabular}{| c | c |}
            \hline
            {{21831}} & {{20}} \\ \hline
            {{553}} & {{29}} \\ \hline
        \end{tabular}
    \end{minipage}
    \hfill
    \begin{minipage}{0.3\textwidth}
        \centering
        \caption*{Κλάση 'effusion' (2)}
        \begin{tabular}{| c | c |}
            \hline
            {{19555}} & {{124}} \\ \hline
            {{2527}} & {{227}} \\ \hline
        \end{tabular}
        \caption*{}
    \end{minipage}


    % --- Row 2 ---
    \vspace{1em}
    \begin{minipage}{0.3\textwidth}
        \centering
        \caption*{Κλάση 'infiltration' (3)}
        \begin{tabular}{| c | c |}
            \hline
            {{18471}} & {{24}} \\ \hline
            {{3916}} & {{22}} \\ \hline
        \end{tabular}
    \end{minipage}
    \hfill
    \begin{minipage}{0.3\textwidth}
        \centering
        \caption*{Κλάση 'mass' (4)}
        \begin{tabular}{| c | c |}
            \hline
            {{21299}} & {{1}} \\ \hline
            {{1133}} & {{0}} \\ \hline
        \end{tabular}
    \end{minipage}
    \hfill
    \begin{minipage}{0.3\textwidth}
        \centering
        \caption*{Κλάση 'nodule' (5)}
        \begin{tabular}{| c | c |}
            \hline
            {{21098}} & {{0}} \\ \hline
            {{1335}} & {{0}} \\ \hline
        \end{tabular}
    \end{minipage}

    \vspace{1em}

    % --- Row 3 ---
    \begin{minipage}{0.3\textwidth}
        \centering
        \caption*{Κλάση 'pneumonia' (6)}
        \begin{tabular}{| c | c |}
            \hline
            {{22191}} & {{0}} \\ \hline
            {{242}} & {{0}} \\ \hline
        \end{tabular}
    \end{minipage}
    \hfill
    \begin{minipage}{0.3\textwidth}
        \centering
        \caption*{Κλάση 'pneumothorax' (7)}
        \begin{tabular}{| c | c |}
            \hline
            {{21344}} & {{0}} \\ \hline
            {{1089}} & {{0}} \\ \hline
        \end{tabular}
    \end{minipage}
    \hfill
    \begin{minipage}{0.3\textwidth}
        \centering
        \caption*{Κλάση 'consolidation' (8)}
        \begin{tabular}{| c | c |}
            \hline
            {{21476}} & {{0}} \\ \hline
            {{957}} & {{0}} \\ \hline
        \end{tabular}
    \end{minipage}

    % --- Row 4 ---
    \vspace{1em}
    \begin{minipage}{0.3\textwidth}
        \centering
        \caption*{Κλάση 'edema' (9)}
        \begin{tabular}{| c | c |}
            \hline
            {{22020}} & {{0}} \\ \hline
            {{413}} & {{0}} \\ \hline
        \end{tabular}
    \end{minipage}
    \hfill
    \begin{minipage}{0.3\textwidth}
        \centering
        \caption*{Κλάση 'emphysema' (10)}
        \begin{tabular}{| c | c |}
            \hline
            {{21924}} & {{0}} \\ \hline
            {{509}} & {{0}} \\ \hline
        \end{tabular}
    \end{minipage}
    \hfill
    \begin{minipage}{0.3\textwidth}
        \centering
        \caption*{Κλάση 'fibrosis' (11)}
        \begin{tabular}{| c | c |}
            \hline
            {{22071}} & {{0}} \\ \hline
            {{362}} & {{0}} \\ \hline
        \end{tabular}
    \end{minipage}

    \vspace{1em}

    % --- Row 5 ---
    \begin{minipage}{0.45\textwidth}
        \centering
        \caption*{Κλάση 'pleural' (12)}
        \begin{tabular}{| c | c |}
            \hline
            {{21699}} & {{0}} \\ \hline
            {{734}} & {{0}} \\ \hline
        \end{tabular}
    \end{minipage}
    \hfill
    \begin{minipage}{0.45\textwidth}
        \centering
        \caption*{Κλάση 'hernia' (13)}
        \begin{tabular}{| c | c |}
            \hline
            {{22391}} & {{0}} \\ \hline
            {{42}} & {{0}} \\ \hline
        \end{tabular}
    \end{minipage}

    \vspace{1em}

    % --- Row 6 ---
    \begin{minipage}{\textwidth}
        \centering
        \caption*{Δείγμα Confusion Matrix για ευκολία}
        \begin{tabular}{| c | c |}
            \hline
            \text{{True Negative}} & \text{{False Positive}} \\ \hline
            \text{{False Negative}} & \text{{True Positive}} \\ \hline
        \end{tabular}
    \end{minipage}
\end{table}


Με βάση τα γενικότερα αποτελέσματα καταλήγουμε στα ακόλουθα συμπεράσματα. Η εκπαίδευση του μοντέλου μας δεν είναι δεδομένο ότι πάντα 
βελτίωνε την επίδοσή του. Ανάλογα με την επιλογή των υπερπαραμέτρων μπορεί η επίδοσή του να γίνεται χειρότερη όσο συνεχίζει η εκπαίδευση.
Ένα παράδειγμα για αυτό το φαινόμενο φαίνεται στην~\autoref{bad_training}, όπου πολύ γρήγορα το accuracy του validation πέφτει, το loss 
του validation ανεβαίνει, ενώ η επίδοση του μοντέλου στο training dataset βελτιώνεται. Είναι πολύ πιθανό το μοντέλο μας να `μάθαινε υπερβολικά καλά', 
να έκανε overfitting στα δεδομένα του training dataset με τον συγκεκριμένο συνδυασμό υπερπαραμέτρων και να μην μπορούσε να γενικεύσει σε άλλη είσοδο.
Από αυτό συμπεραίνουμε ότι η επιλογή των σωστών υπερπαραμέτρων είναι πολύ σημαντική και επηρεάζει την επίδοση του μοντέλου μας. Τέλος, η κανονικοποίηση εισόδου
δεν επηρέασε ιδιαίτερα την επίδοση του μοντέλου. Στο~\autoref{best_hyper_comb_no_input_norm} φαίνεται ο καλύτερος συνδυασμός υπερπαραμέτρων χωρίς κανονικοποίηση, 
ο οποίος πέτυχε 0.9495 validation accuracy και 0.1609 validation loss στο epoch 28, αριθμοί πολύ ανταγωνιστικοί σε σχέση με τον καλύτερο συνδυασμό υπερπαραμέ\-τρων 
με κανονικοποίηση εισόδου (και γενικά) ο οποίος είχε 0.9496 validation accuracy και 0.1602 validation loss. 


\begin{figure}
    \includegraphics[width=\linewidth]{images/input_norm_yes_layer_norm_bad.png}
    \caption{Παράδειγμα κακής γενίκευσης μοντέλου και overfitting στο training dataset}
    \label{bad_training}
\end{figure}

\begin{figure}
    \includegraphics[width=\linewidth]{images/no_input_norm_plot_best.png}
    \caption{Καλύτερος συνδυασμός χωρίς κανονικοποίηση εισόδου}
    \label{best_hyper_comb_no_input_norm}
\end{figure}

\section{Transfer Learning}
Στο δεύτερο μέρος της εργασίας καλούμαστε να χρησιμοποιήσουμε ένα pre-trained model και να εξετάσουμε πότε αξίζει να κάνουμε feature extraction και πότε 
fine-tuning. Για αυτό τον σκοπό επιλέχθηκε το μοντέλο MobileNetV2 λόγω της μικρών υπολογιστικών απαιτήσεών του.

\subsection{Προετοιμασία του Dataset}
Ο κώδικας για την προετοιμασία του dataset, ορισμού seeds για αναπαραγωγιμότητα και των συναρτήσεων train\_one\_epoch, evaluate\_model και test\_model 
παραμένει επί το πλείστον ο ίδιος με τον κώδικα του πρώτου μέρους.
Η μόνη διαφορά είναι στο ότι οι εικόνες της εισόδου μετατρέπονται από ασπρόμαυρες σε `έγχρωμες', δηλαδή αντί για ένα κανάλι φωτεινότητας κάθε εικόνα έχει τρία κανάλια. Αυτό 
είναι απαίτηση του μοντέλου που επιλέξαμε καθώς περιμένει έγχρωμες εικόνες στην είσοδό του. Το batch size εξακολουθεί να είναι 64.

\subsection{Περιγραφή Pre-Trained Μοντέλου}

% Αντεστραμμένα Υπολειμματικά Μπλοκ (Inverted Residual Blocks): Σε αντίθεση με τα παραδοσιακά υπολειμματικά μπλοκ που επεκτείνουν τα χαρακτηριστικά και στη συνέχεια 
% τα προβάλλουν προς τα κάτω, τα αντεστραμμένα υπολειμματικά μπλοκ της MobileNetV2 αρχικά επεκτείνουν τα χαρακτηριστικά εισόδου χρησιμοποιώντας μια συνέλιξη 1x1 
% (σε υψηλότερη διάσταση), στη συνέχεια εφαρμόζουν μια συνέλιξη βάθους (depthwise convolution, για ελαφριά χωρική φιλτράριση) και τέλος τα προβάλλουν πίσω σε χαμηλότερη 
% διάσταση χρησιμοποιώντας μια άλλη συνέλιξη 1x1.

% Γραμμικά Σημεία Συμφόρησης (Linear Bottlenecks): Οι συνελίξεις προβολής 1x1 (το τελικό επίπεδο στο αντεστραμμένο υπολειμματικό μπλοκ) σκόπιμα δεν χρησιμοποιούν μη 
% γραμμική συνάρτηση ενεργοποίησης (όπως η ReLU). Αυτό είναι ζωτικής σημασίας για τη διατήρηση πληροφοριών σε χαμηλές διαστάσεις, αποτρέποντας την απώλεια σημαντικών χαρακτηριστικών που θα μπορούσε να συμβεί αν εφαρμοζόταν η ReLU μετά την προβολή.

% Συνελίξεις Βάθους Διαχωρίσιμες (Depthwise Separable Convolutions): Ένα θεμελιώδες δομικό στοιχείο που κληρονομήθηκε από την MobileNetV1, αυτές οι συνελίξεις μειώνουν 
% σημαντικά τον υπολογισμό χωρίζοντας τις τυπικές συνελίξεις σε δύο βήματα: μια συνέλιξη βάθους (εφαρμόζοντας ένα μόνο φίλτρο ανά κανάλι εισόδου) και μια συνέλιξη σημείου 
% (μια συνέλιξη 1x1 για τον συνδυασμό των εξόδων της λειτουργίας βάθους).

% Αποδοτικότητα: Ο συνδυασμός των αντεστραμμένων υπολειμματικών μπλοκ, των γραμμικών σημείων συμφόρησης και των συνελίξεων βάθους διαχωρίσιμων οδηγεί σε ένα πολύ ελαφρύ και γρήγορο μοντέλο, καθιστώντας το ιδανικό για συσκευές με περιορισμένους υπολογιστικούς πόρους, όπως smartphones και συσκευές IoT.
% Μεταφορά Μάθησης (Transfer Learning): Λόγω της αποδοτικότητάς της και της καλής απόδοσής της σε μεγάλα σύνολα δεδομένων όπως το ImageNet, η MobileNetV2 είναι επίσης μια εξαιρετική επιλογή για μεταφορά μάθησης, επιτρέποντας την προσαρμογή της σε νέες εργασίες όρασης υπολογιστών με σχετικά μικρά σύνολα δεδομένων.

Αρχικά φορτώνουμε το μοντέλο MobileNetV2 με σκοπό το feature extraction και παγώνουμε όλες τις παραμέ\-τρους του. Αντικαθιστούμε το τελευταίο Linear 
layer του classifier με ένα καινούργιο Linear layer το οποίο έχει τον ίδιο αριθμό in features αλλά μόνο 14 out features, εφόσον έχουμε 14 κλάσεις.
Οι παράμετροι του νέου layer δεν είναι παγωμένοι, οπότε μόνο αυτό το layer θα εκπαιδευτεί. Τέλος δίνουμε στον Adam optimizer τις παραμέτρους του νέου
classifier head. Σε αυτή τη μορφή το μοντέλο εκπαιδεύτηκε για 20 epochs
με learning rate $10^3$. Μετά το training και validation του παραπάνω μοντέλου ξεπαγώνουμε και τα δύο τελευταία 
συνελικτικά blocks του μοντέλου. Δίνουμε τις κατάλληλες παραμέτρους στον Adam Optimizer και εκπαιδεύουμε με batch size 32 για άλλα 15 epochs.
Με αυτόν τον τρόπο θα δούμε αν το fine-tuning έχει θετική επίδραση στην επίδοση του μοντέλου μας στο συγκεκριμένο πρόβλημα.

\subsection{Αποτελέσματα και Συμπεράσματα}
Στο~\autoref{plot_feature_extraction_fine_tuning} και στον~\autoref{table__feature_extraction_fine_tuning} φαίνονται τα αποτελέσματα του training και του validation
του μοντέλου. Βλέπουμε ότι αφότου άρχισε το fine-tuning (μετά το epoch 20) υπάρχει μείωση του training loss και μικρή αύξηση του training accuracy, άρα μπορούμε να πούμε
ότι το fine-tuning βοήθησε το μοντέλο να ταιριάξει καλύτερα στα training δεδομένα. Το validation loss επίσης μειώνεται, ειδικά στα πρώτα epochs του fine-tuning training. 
Στα τελευταία epochs του fine-tuning training όμως το validation loss αυξάνεται λίγο, γεγονός που ίσως υποδεικνύει ότι το μοντέλο αρχίζει να χάνει τη δυνατότητα
να γενικεύει πέρα από το training dataset και να τείνει προς overfitting. Το validation accuracy αυξάνεται πολύ λίγο στην αρχή του fine-tuning αλλά μετά μένει σταθερό. 
Άρα το μοντέλο μας γενικεύει λίγο καλύτερα με τη χρήση fine-tuning, αλλά αυτή η βελτίωση δεν είναι σημαντική, ειδικά αν λάβουμε υπόψιν ότι το validation accuracy μένει κυρίως σταθερό.
Συνεπώς, μπορούμε να πούμε ότι το feature extraction ήταν επαρκής μέθοδος για το παρών πρόβλημα. Γενικεύοντας, μπορούμε να πούμε ότι το feature extraction είναι προτιμότερη
επιλογή σε περιπτώσεις όπου το dataset μας είναι σχετικά μικρό, έχουμε περιορισμένο χρόνο και υπολογιστικούς πόρους και όπου τα δεδομένα με τα οποία εκπαιδεύτηκε
το pre-trained μοντέλο ήταν παρόμοιας φύσης με τα δεδομένα της δικής μας εφαρμογής. Από την άλλη, το fine-tuning είναι κατάλληλο όταν έχουμε μεγάλο dataset, 
όταν θέλουμε καλύτερο performance από αυτό που πετυχαίνουμε μόνο με feature extraction και όταν τα δεδομένα του προβλήματός μας είναι ελαφρώς διαφορετικά από τα δεδομένα
με τα οποία εκαπιδεύτηκε το pre-trained μοντέλο.


\begin{figure}
    \includegraphics[width=0.9\linewidth]{images/fine_tuning_feature_extraction.png}
    \caption{Σύγκριση Feature Extraction με Fine-Tuning. Αριστερά της κόκκινης διακεκομμένης γραμμής είναι το μοντέλο με feature extraction μόνο και δεξιά της είναι το μοντέλο
    και με fine-tuning.}\label{plot_feature_extraction_fine_tuning}
\end{figure}

\begin{table}
    \begin{tabular}{lcc}
        \toprule
        Μετρική & Μετά το τέλος Feature Extraction (Epoch 20) & Μετά το τέλος Fine Tuning \\
        \midrule
        Train Loss & 0.1849 & 0.1698 \\
        Val Loss & 0.1802 &	0.1737 \\
        Train Accuracy & 0.9480 & 0.9486 \\
        Val Accuracy & 0.9492 & 0.9492 \\
        \bottomrule
    \end{tabular}
    \caption{Σύγκριση μετρικών για Feature Extraction και Fine-Tuning}\label{table__feature_extraction_fine_tuning}
\end{table}

Το μοντέλο πέτυχε 0.9475 test accuracy. Παρατηρώντας τα confusion matrices του~\autoref{confusion_matrices_part2} βλέπουμε ότι το μοντέλο
μας έχει τα ίδια προβλήματα με το μοντέλο του πρώτου μέρους. Το μοντέλο δίνει προτεραιότητα στο 
να ανιχνεύει την απώλεια ασθένειας, με αποτέλεσμα υψηλούς αριθμούς True και False Negatives και
πολύ μικρούς αριθμούς True και False Positives. Δεν μπορούμε να πούμε ότι το μοντέλο μας είναι 
πολύ καλύτερο από άποψη απόδοσης απο το μοντέλο του πρώτου μέρους, μπορούμε όμως να πούμε 
ότι η εκπαίδευση του μοντέλου του πρώτου μέρους και η εύρεση των κατάλληλων υπερπαραμέτρων
ήταν πολύ πιο χρονοβόρα διαδικασία από τις μεθόδους fine tuning και feature extraction.

\begin{table}
    \centering
    \caption{Confusion Matrices για όλες τις κλάσεις για το δεύτερο μέρος.}\label{confusion_matrices_part2}
    
    % --- Row 1 ---
    \begin{minipage}{0.3\textwidth}
        \centering
        \caption*{Κλάση 'atelectasis' (0)}
        \begin{tabular}{| c | c |}
            \hline
            20013 & 0 \\ \hline
            2420 & 0 \\ \hline
        \end{tabular}
    \end{minipage}
    \hfill
    \begin{minipage}{0.3\textwidth}
        \centering
        \caption*{Κλάση 'cardiomegaly' (1)}
        \begin{tabular}{| c | c |}
            \hline
            21851 & 0 \\ \hline
            582 & 0 \\ \hline
        \end{tabular}
    \end{minipage}
    \hfill
    \begin{minipage}{0.3\textwidth}
        \centering
        \caption*{Κλάση 'effusion' (2)}
        \begin{tabular}{| c | c |}
            \hline
            19670 & 9 \\ \hline
            2733 & 21 \\ \hline
        \end{tabular}
    \end{minipage}

    \vspace{1em}

    % --- Row 2 ---
    \begin{minipage}{0.3\textwidth}
        \centering
        \caption*{Κλάση 'infiltration' (3)}
        \begin{tabular}{| c | c |}
            \hline
            18487 & 8 \\ \hline
            3928 & 10 \\ \hline
        \end{tabular}
    \end{minipage}
    \hfill
    \begin{minipage}{0.3\textwidth}
        \centering
        \caption*{Κλάση 'mass' (4)}
        \begin{tabular}{| c | c |}
            \hline
            21300 & 0 \\ \hline
            1133 & 0 \\ \hline
        \end{tabular}
    \end{minipage}
    \hfill
    \begin{minipage}{0.3\textwidth}
        \centering
        \caption*{Κλάση 'nodule' (5)}
        \begin{tabular}{| c | c |}
            \hline
            21098 & 0 \\ \hline
            1335 & 0 \\ \hline
        \end{tabular}
    \end{minipage}

    \vspace{1em}

    % --- Row 3 ---
    \begin{minipage}{0.3\textwidth}
        \centering
        \caption*{Κλάση 'pneumonia' (6)}
        \begin{tabular}{| c | c |}
            \hline
            22191 & 0 \\ \hline
            242 & 0 \\ \hline
        \end{tabular}
    \end{minipage}
    \hfill
    \begin{minipage}{0.3\textwidth}
        \centering
        \caption*{Κλάση 'pneumothorax' (7)}
        \begin{tabular}{| c | c |}
            \hline
            21344 & 0 \\ \hline
            1089 & 0 \\ \hline
        \end{tabular}
    \end{minipage}
    \hfill
    \begin{minipage}{0.3\textwidth}
        \centering
        \caption*{Κλάση 'consolidation' (8)}
        \begin{tabular}{| c | c |}
            \hline
            21476 & 0 \\ \hline
            957 & 0 \\ \hline
        \end{tabular}
    \end{minipage}

    \vspace{1em}

    % --- Row 4 ---
    \begin{minipage}{0.3\textwidth}
        \centering
        \caption*{Κλάση 'edema' (9)}
        \begin{tabular}{| c | c |}
            \hline
            22020 & 0 \\ \hline
            413 & 0 \\ \hline
        \end{tabular}
    \end{minipage}
    \hfill
    \begin{minipage}{0.3\textwidth}
        \centering
        \caption*{Κλάση 'emphysema' (10)}
        \begin{tabular}{| c | c |}
            \hline
            21924 & 0 \\ \hline
            509 & 0 \\ \hline
        \end{tabular}
    \end{minipage}
    \hfill
    \begin{minipage}{0.3\textwidth}
        \centering
        \caption*{Κλάση 'fibrosis' (11)}
        \begin{tabular}{| c | c |}
            \hline
            22071 & 0 \\ \hline
            362 & 0 \\ \hline
        \end{tabular}
    \end{minipage}

    \vspace{1em}

    % --- Row 5 ---
    \begin{minipage}{0.45\textwidth}
        \centering
        \caption*{Κλάση 'pleural' (12)}
        \begin{tabular}{| c | c |}
            \hline
            21699 & 0 \\ \hline
            734 & 0 \\ \hline
        \end{tabular}
    \end{minipage}
    \hfill
    \begin{minipage}{0.45\textwidth}
        \centering
        \caption*{Κλάση 'hernia' (13)}
        \begin{tabular}{| c | c |}
            \hline
            22391 & 0 \\ \hline
            42 & 0 \\ \hline
        \end{tabular}
    \end{minipage}

    \vspace{1em}

    % --- Row 6 ---
    \begin{minipage}{\textwidth}
        \centering
        \caption*{Δείγμα Confusion Matrix για ευκολία}
        \begin{tabular}{| c | c |}
            \hline
            True Negative & False Positive \\ \hline
            False Negative & True Positive \\ \hline
        \end{tabular}
    \end{minipage}
\end{table}


\section{Small Vision Transformer}
Στο τρίτο και τελευταίο μέρος της εργασίας καλούμαστε να χρησιμοποιήσουμε έναν pre-trained Vision Transfor\-mer. Ο transformer που 
χρησιμοποιήθηκε είναι ο DeiT-tiny.

\subsection{Προετοιμασία του Dataset}
Η προετοιμασία δεδομένων είναι πολύ παρόμοια με την προετοιμασία για το δεύτερο μέρος. Η πιο σημασντική διαφορά είναι ότι ο DeiT-tiny, εκτός από
έγχρωμες εικόνες, αναμένει εικόνες εισόδου μεγέθους $224\times{}224$. Για αυτό λόγο χρειάστηκε να κάνουμε resize τις εικόνες του ChestMNIST dataset. 
Επίσης ο transformer εκπαιδεύτηκε με συγκεκριμένη κανονικοποίηση στα δεδομένα
εισόδου, με διαφορετική μέση τιμή και τυπική απόκλιση για κάθε κανάλι χρώματος. Αυτή την κανονικοποίηση χρησιμοποιήσαμε και εμείς στα δικά μας
δεδομένα εισόδου. 



\subsection{Training, Validation, Testing}
Με παρόμοιο τρόπο με το δεύτερο μέρος, πρώτα κάνουμε feature extraction στον transformer, χρησιμοποιώντας learning rate $10^3$, batch size $32$ και
εκπαιδεύοντας το νέο classifier head για $15$ epochs. Έπειτα ξεπαγώνουμε τα δύο τελευταία layers του transformer, αλλάζουμε σε batch size $16$ και
συνεχίζουμε την εκπαίδευση του μοντέλου για άλλα $10$ epochs.

\subsection{Αποτελέσματα και Συμπεράσματα}
Στο~\autoref{transformer_plot} βλέπουμε τα αποτελέσματα του feature extraction και του fine tuning. Όσον αφορά το feature extraction, το validation 
accuracy αρχικά αυξάνει και πολύ γρήγορα σταθεροποιείται. Αντίστοιχα το validation loss αρχικά μειώνεται και σύντομα σταθεροποιείται. Συνεπώς ο
transformer πολύ γρήγορα προσαρμόστηκε στο δικό μας πρόβλημα μετά το pre-training. Η γρήγορη σύγκλιση του transformer στη φάση feature extraction 
οφείλεται στα δυνατά pre-trained features (το DeiT-tiny εκπαιδεύτηκε στο ImageNet). Στη φάση fine tuning αρχικά βλέπουμε παρόμοια συμπεριφορά, 
αρχική αύξηση του validation accuracy και μείωση του validation loss. Όσο πλησιάζουμε στο τέλος του fine tuning training βλέπουμε ότι το 
validation accuracy αρχίζει να πέφτει και το validation loss να αρχίσει να ανεβαίνει, ενώ τα training accuracy και loss συνεχίζουν την πορεία
τους και υποδεικνύουν ότι το μοντέλο συνεχίζει να βελτιώνει την επίδοσή του στο training dataset. Πιθανώς ο transformer να είχε αρχίσει να κάνει
overfitting στο training dataset σε εκείνο το σημείο.

\begin{figure}
    \includegraphics[width=0.9\linewidth]{images/transformer_plot.png}
    \caption{Plots transformer για training και validation. Fine Tuning ξεκίνησε μετά το epoch 15.}\label{transformer_plot}
\end{figure}

Ο transformer πέτυχε test accuracy 0.9454. Παρατηρώντας τα confusion matrices στον~\autoref{transformer_confusion_matrices}, μπορούμε να δούμε
ότι η ανισορροπία των κλάσεων πάλι επηρέασε αρνητικά την επίδοση του μοντέλου μας. Ειδικά στις πιο μειονοτικές κλάσεις παθήσεων όπως `pneumonia' και
`consolidation', βλέπουμε τα ίδια προβλήματα που παρατηρήσαμε στα προηγούμενα δύο μέρη τις εργασίας, πολύ υψηλά ποσοστά True και False Negatives και 
μηδενικά, ή σχεδόν μηδενικά True Positives. Παρατηρούμε δηλαδή τη συντηριτικότητα του μοντέλου να προβλέψει τις ίδιες παθήσεις και την προτίμησή του
να προβλέπει την απώλεια παθήσεων. Παρόλα αυτά σε κλάσεις με μεγαλύτερο αριθμό δειγμάτων όπως `effusion' και `infiltration' βλέπουμε ότι υπάρχουν 
περισσότεροι αριθμοί True Positives, δηλαδή το μοντέλο μας κατάφερε να ανιχνεύσει τις ασθένειες αρκετές φορές. Σε σχέση με τα False Negatives όμως
οι αριθμοί αυτοί παραμένουν μικροί. Παρόλα αυτά, αυτό είναι μια βελτίωση από τα προηγούμενα δύο μοντέλα.
\begin{table}
    \centering
    \caption{Transformer Confusion Matrices για όλες τις κλάσεις.}\label{transformer_confusion_matrices}
    
    % --- Row 1 ---
    \begin{minipage}{0.3\textwidth}
        \centering
        \caption*{Κλάση 'atelectasis' (0)}
        \begin{tabular}{| c | c |}
            \hline
            19842 & 171 \\ \hline
            2326 & 94 \\ \hline
        \end{tabular}
    \end{minipage}
    \hfill
    \begin{minipage}{0.3\textwidth}
        \centering
        \caption*{Κλάση 'cardiomegaly' (1)}
        \begin{tabular}{| c | c |}
            \hline
            21760 & 91 \\ \hline
            545 & 37 \\ \hline
        \end{tabular}
    \end{minipage}
    \hfill
    \begin{minipage}{0.3\textwidth}
        \centering
        \caption*{Κλάση 'effusion' (2)}
        \begin{tabular}{| c | c |}
            \hline
            18868 & 811 \\ \hline
            2081 & 673 \\ \hline
        \end{tabular}
    \end{minipage}

    \vspace{1em}

    % --- Row 2 ---
    \begin{minipage}{0.3\textwidth}
        \centering
        \caption*{Κλάση 'infiltration' (3)}
        \begin{tabular}{| c | c |}
            \hline
            18112 & 383 \\ \hline
            3659 & 279 \\ \hline
        \end{tabular}
    \end{minipage}
    \hfill
    \begin{minipage}{0.3\textwidth}
        \centering
        \caption*{Κλάση 'mass' (4)}
        \begin{tabular}{| c | c |}
            \hline
            21158 & 142 \\ \hline
            1075 & 58 \\ \hline
        \end{tabular}
    \end{minipage}
    \hfill
    \begin{minipage}{0.3\textwidth}
        \centering
        \caption*{Κλάση 'nodule' (5)}
        \begin{tabular}{| c | c |}
            \hline
            21061 & 37 \\ \hline
            1332 & 3 \\ \hline
        \end{tabular}
    \end{minipage}

    \vspace{1em}

    % --- Row 3 ---
    \begin{minipage}{0.3\textwidth}
        \centering
        \caption*{Κλάση 'pneumonia' (6)}
        \begin{tabular}{| c | c |}
            \hline
            22191 & 0 \\ \hline
            242 & 0 \\ \hline
        \end{tabular}
    \end{minipage}
    \hfill
    \begin{minipage}{0.3\textwidth}
        \centering
        \caption*{Κλάση 'pneumothorax' (7)}
        \begin{tabular}{| c | c |}
            \hline
            21247 & 97 \\ \hline
            1053 & 36 \\ \hline
        \end{tabular}
    \end{minipage}
    \hfill
    \begin{minipage}{0.3\textwidth}
        \centering
        \caption*{Κλάση 'consolidation' (8)}
        \begin{tabular}{| c | c |}
            \hline
            21441 & 35 \\ \hline
            952 & 5 \\ \hline
        \end{tabular}
    \end{minipage}

    \vspace{1em}

    % --- Row 4 ---
    \begin{minipage}{0.3\textwidth}
        \centering
        \caption*{Κλάση 'edema' (9)}
        \begin{tabular}{| c | c |}
            \hline
            21993 & 27 \\ \hline
            406 & 7 \\ \hline
        \end{tabular}
    \end{minipage}
    \hfill
    \begin{minipage}{0.3\textwidth}
        \centering
        \caption*{Κλάση 'emphysema' (10)}
        \begin{tabular}{| c | c |}
            \hline
            21890 & 34 \\ \hline
            505 & 4 \\ \hline
        \end{tabular}
    \end{minipage}
    \hfill
    \begin{minipage}{0.3\textwidth}
        \centering
        \caption*{Κλάση 'fibrosis' (11)}
        \begin{tabular}{| c | c |}
            \hline
            22068 & 3 \\ \hline
            362 & 0 \\ \hline
        \end{tabular}
    \end{minipage}

    \vspace{1em}

    % --- Row 5 ---
    \begin{minipage}{0.45\textwidth}
        \centering
        \caption*{Κλάση 'pleural' (12)}
        \begin{tabular}{| c | c |}
            \hline
            21685 & 14 \\ \hline
            732 & 2 \\ \hline
        \end{tabular}
    \end{minipage}
    \hfill
    \begin{minipage}{0.45\textwidth}
        \centering
        \caption*{Κλάση 'hernia' (13)}
        \begin{tabular}{| c | c |}
            \hline
            22391 & 0 \\ \hline
            42 & 0 \\ \hline
        \end{tabular}
    \end{minipage}

    \vspace{1em}

    % --- Row 6 ---
    \begin{minipage}{\textwidth}
        \centering
        \caption*{Δείγμα Confusion Matrix για ευκολία}
        \begin{tabular}{| c | c |}
            \hline
            True Negative & False Positive \\ \hline
            False Negative & True Positive \\ \hline
        \end{tabular}
    \end{minipage}
\end{table}

Σαν τελικά συμπεράσματα όλων των αποτελεσμάτων ως τώρα, μπορούμε να συμπαράνουμε τα εξής. Τα CNN from scratch έχουν τη δυσκολία ότι η αρχιτεκτονική τους πρέπει να σχεδιαστεί από την αρχή. Η σύγκλισή τους
είναι αργή εφόσον εκπαιδεύονται from scratch και η επίδοσή τους εξαρτάται σε μεγάλο βαθμό από την επιλογή βέλτιστων υπερπαραμέτρων, η εύρεση των οποίων απαιτεί χρονοβόρο πειραματισμό. Για να
πετύχουν επιδόσεις ανταγωνιστικές των pre-trained μοντέλων είναι απαραίτητη η ύπαρξη πολύ μεγάλου training dataset, αλλιώς υπάρχει αυξημένος κίνδυνος overfitting. Στο ChestMNIST dataset, 
λόγω της ύπαρξης μεγάλης ανισορροπίας κλάσεων και περιορισμένου αριθμού δειγμάτων, η επίδοση του CNN from scratch δεν ήταν καλή, όπως είδαμε από τα confusion matrices του test dataset.
Οι pre-trained transformers έχουν γρήγορη σύγκλιση και στο feature extraction και στο fine tuning λόγω των pre-trained features τους. Η γενίκευσή τους είναι καλή, εφόσον έχουν 
εκπαιδευτεί σε επαρκώς μεγάλο dataset. Στο δικό μας πρόβλημα, ο DeiT-tiny δυσκολεύτηκε μεν, λόγω της ανισορροπίας κλάσεων, να γενικεύσει καλά, αλλα σίγουρα είχε καλύτερη γενίκευση από τo 
CNN from scratch και το transfer learning CNN. Τέλος, το CNN με transfer learning (MobileNetV2) στο δικό μας πρόβλημα είχε παρόμοια γενίκευση με το CNN from scratch, παρόλο που 
το pre-training του έγινε στο ImageNet, το ίδιο dataset με το DeiT-tiny. Πιθανώς η αρχιτεκτονική του MobileNetV2 να μην είναι ταιριαστή στο δικό μας πρόβλημα και να ήταν 
ιδιαίτερα ευαίσθητη στην ανισορροπία κλάσεων του ChestMNIST. Παρόλα αυτά, σε σχέση με το CNN from scratch, η σύγκλισή του ήταν ταχύτερη.

\subsection{Αρχιτεκτονική DeiT-tiny Transformer}

Ο DeiT-tiny (Data-efficient Image Transformer - Tiny) είναι μια παραλλαγή του Vision
Transformer (ViT), σχεδιασμένη να λειτουργεί αποτελεσματικά χωρίς τα τεράστια
pre-training datasets που συνήθως απαιτούν οι transformers. Κύρια χαρακτηριστικά της αρχιτεκτονικής
τους είναι τα εξής. Οι εικόνες εισόδου μεγέθους $224\times224$ χωρίζονται σε σταθερού μεγέθους patches $16\times16$ τα οποία 
γίνονται flatten και προβάλλονται γραμμικά σε διανύσματα
embedding σταθερού μήκους (patch tokens). $196$ patch tokens 
προστίθενται μετά από ένα class token, το οποίο είναι ένα learnable διάνυσμα που προσπαθεί να
προβλέψει την κλάση στην οποία ανήκει η εικόνα εισόδου, το ground truth. 

Στο τέλος αυτού του sequence από tokens
προστίθεται το distillation token, χαρακτη\-ριστικό της αρχιτεκτονικής DeiT. Αυτό είναι
learnable και αλληλεπιδρά με όλα τα υπόλοιπα tokens στη σειρά για να εκπαιδευτεί από
ένα teacher μοντέλο (συνήθως CNN). Αυτό το token ουσιαστικά προσπαθεί να μαντέψει την πρόβλεψη του CNN για την 
εικόνα εισόδου. Η χρήση του teacher CNN και του distillation token κατά τη διάρκεια της εκπαίδευσης επιτρέπει στο μοντέλο να 
επωφεληθεί από τα inductive biases (όπως η αμεταβλητότητα στη μετατόπιση, ή translation
invariance) που είναι εγγενείς στα συνελικτικά επίπεδα και να `κληρονομήσει' τη χωρική αντίληψη ενός CNN. Αυτό το γεγονός το βοηθά
να γενικεύει καλύτερα όταν τα δεδομένα είναι περιορισμένα, καθώς δεν χρειάζεται να μάθει θεμελιώδη αξιώματα χωρικής συσχέτισης στις εικόνες από έξτρα δεδομένα. 
Για αυτό το λόγο, σε αντίθεση με τα αρχικά ViTs που απαιτούν εκατοντάδες εκατομμύρια εικόνες, το DeiT-tiny μπορεί να επιτύχει
ανταγωνιστικά αποτελέσματα χρησιμοποιώντας μικρότερου μεγέθους datasets. Παρόλα αυτά, η χρήση του distillation token πολλές φορές
δεν είναι αρκετή και εξαιρετικά μικρά ή εξειδικευμένα σύνολα δεδομένων ενδέχεται να δυσκολέψουν το DeiT να μάθει την τοπική
δομή όσο αποτελεσματικά όσο ένα απλό CNN. 

Επειδή οι transformers δεν έχουν αντίληψη της χωρικής σειράς, προστίθενται 
absolute positional embeddings σε κάθε ένα από τα $198$ συνολικά token της σειράς, με σκοπό τη διατήρηση
της χωρικής πληροφορίας πριν από την επεξεργασία. Αυτή είναι η είσοδος του transformer encoder, ο οποίος χρησιμοποιεί
μηχανισμούς self-attention για τον υπολογισμό των global σχέσεων μεταξύ όλων των tokens. Η έκδοση 
"tiny" χρησιμοποιεί συγκεκριμένα $12$ layers με $3$ attention heads και embedding διανύσματα με διάσταση $192$.  

Στο πλαίσιο των vision transformers το self-attention είναι ένας μηχανισμός που επιτρέπει σε κάθε patch ενός sequence να
`κοιτάει' όλα τα υπόλοιπα patch της ίδιας εικόνας ταυτόχρονα, έτσι ώστε να εντοπίσει ποιά patch είναι σημαντικά για τη 
δική του αναπαράσταση. Ένα attention-head είναι ένα ανεξάρτητο instance self-attention. Οι transformers χρησιμοποιούν 
Multi-Head Attention (MHA), το οποίο τρέχει πολλά attention heads παράλληλα. Κάθε attention head εκπαιδεύεται να
εστιάζει σε διαφορετικές πτυχές των δεδομένων (τοπικές ακμές, υφές, κλπ). Οι έξοδοι των attention heads 
κατατάσσονται σε μια σειρά και περνάνε από το τελικό γραμμικό layer. Αυτή είναι και η έξοδος του transformer block.

Το DeiT-tiny είναι εξαιρετικά αποδοτικό και απαιτεί σημαντικά λιγότερους υπολογιστικούς
πόρους και χρόνο εκπαίδευσης σε σχέση με τα μεγαλύτερα μοντέλα transformer, εφόσον αποτελείται από περίπου $5.7$ εκατομμύρια παραμέτρους μόνο.
Για σύγκριση, το DeiT-small αποτελείται από $22$ εκατομμύρια παραμέτρους και το DeiT-base από $86$ εκατομμύρια παραμέτρους. Παρόλα αυτά, είναι ακόμα
απαιτητικό σε υπολογιστικούς πόρους σε σχέση με ένα CNN. Επίσης, παρά το σχετικά μικρό μέγεθός του, ο DeiT-tiny είναι επιρρεπής σε over-fitting σε μικρά σύνολα δεδομένων.
Τέλος, η απόδοσή του μπορεί να μειωθεί σημαντικά αν η εικόνες εισόδου έχουν διαφορετικές διαστάσεις από $224\times{}224$.


\section{Προτάσεις Βελτίωσης}
Με βάση τα παραπάνω, υπάρχουν αρκετές κατευθύνσεις για μελλοντική επέκταση της εργασίας. Ανεξαρτήτως από τις μεθόδους που χρησιμοποιήθηκαν:
\begin{itemize}
    \item \textbf{Weighted Loss Function:} Η υλοποίηση μιας συνάρτησης απώλειας με βάρη που θα ευνοούν μεινοτικές κλάσεις (που είναι η πλειοψηφία των κλάσεων
        με πάθηση) θα ωθούσε το μοντέλο να προβλέπει και τις ίδιες τις παθήσεις, αντί να προβλέπει μόνο την απώλειά τους.
    \item \textbf{Βελτιστοποίηση Κατωφλίου Απόφασης:} Ο πειραματισμός με το σταθερό κατώφλι της σιγμοει\-δούς συνάρτησης που μετατρέπει τα logits σε
        δυαδική πιθανότητα \verb|torch.sigmoid(outputs) > 0.5| μπορεί να βοηθήσει στην ανίχνευση των παθήσεων.
    \item \textbf{Αντιμετώπιση Ανισορροπίας Κλάσεων:} Στις μειονοτικές κλάσεις θα μπορούσε να χρησιμοποι\-ηθεί data agumentation, ή κάποιου άλλου είδους oversampling
        ενώ στις πλειοψηφικές κλάσεις θα μπο\-ρούσε να χρησιμοποιηθεί undersampling, για την καταπολέμηση της ανισορροπίας των κλάσεων στο ίδιο το dataset.
\end{itemize}

Προτάσεις βελτίωσης για την υλοποίηση του CNN με εκπαίδευση από την αρχή:
\begin{itemize}
    \item \textbf{Επιλογή βέλτιστων υπερπαραμέτρων με άλλη μετρική:} Επιλογή βέλτιστου συνδυασμού υπερπαραμέτρων με βάση μετρικές όπως F1 score, 
        όχι του accuracy, λόγω ανισορροπίας κλάσεων, πιθανώς να μας έδινε δαφορετική εικόνα για το ποιός συνδυασμός παραμέτρων είναι ο
        κατάλληλος για το παρών προόβλημα.
    \item \textbf{Δοκιμές περισσότερων συνδυασμών υπερπαραμέτρων:} Σε συνδυασμό με το παραπάνω, υπερπαράμετροι που αποκλείστηκαν όπως μερικές
        τιμές των weight decay και dropout, καθώς και η κανονικοποίηση σε επίπεδο layer καλό θα ήταν να επαναφερθούν στις δοκιμές.
\end{itemize}

Προτάσεις βελτίωσης για transfer learning και feature extraction σε CNN και σε vision transformer:
\begin{itemize}
    \item \textbf{Δοκιμή διαφορετικού pre-trained μοντέλου}: Χρήση pre-trained μοντέλου με καλύτερη επίδοση σε πιο περίπλοκα προβλήματα 
        (VGG16, ResNet18, DeiT-small), εφόσον υπάρχουν οι πόροι.
    \item \textbf{Κανονικοποίση εισόδου με βάση στατιστικά της εισόδου:} Σε αντίθεση με το τρίτο μέρος, στο δεύτερο δεν χρησιμοποιήθηκαν
        τα στατιστικά χαρακτηριστικά του ImageNet για την κανονικο\-ποί\-ηση της εισόδου, αλλά σταθερές υπερπαράμετροι. Ίσως η 
        αλλαγή της κανονικοποίησης εισόδου όπως είναι στο τρίτο μέρος να βοηθούσε την επίδοση του μοντέλου.
    \item \textbf{Χρήση εικόνων με μεγαλύτερη ανάλυση:} Η αρχική ανάλυση των εικόνων $28\times{}28$ είναι πολύ μικρή σε σχέση με το $16\times{}16$ patch 
        του DeiT-tiny. Παρόλο που μεγενθύναμε τις φωτογραφίες στο μέγεθος για το οποίο έχει βελτιστοποιηθεί ο DeiT-tiny, υπάρχει περίπτωση να 
        είχαμε περισσότερη πληροφορία που θα ήταν χρήσιμη στην ανίχνευση των παθήσεων αν οι εικόνες του dataset είχαν μεγαλύτερη αρχική ανάλυση.
\end{itemize}


\vspace{3em}
\centering
\emph{*** ΤΕΛΟΣ ΑΝΑΦΟΡΑΣ ***}
\end{document}
